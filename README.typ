// Some definitions presupposed by pandoc's typst output.
#let blockquote(body) = [
  #set text( size: 0.92em )
  #block(inset: (left: 1.5em, top: 0.2em, bottom: 0.2em))[#body]
]

#let horizontalrule = line(start: (25%,0%), end: (75%,0%))

#let endnote(num, contents) = [
  #stack(dir: ltr, spacing: 3pt, super[#num], contents)
]

#show terms: it => {
  it.children
    .map(child => [
      #strong[#child.term]
      #block(inset: (left: 1.5em, top: -0.4em))[#child.description]
      ])
    .join()
}

// Some quarto-specific definitions.

#show raw.where(block: true): set block(
    fill: luma(230),
    width: 100%,
    inset: 8pt,
    radius: 2pt
  )

#let block_with_new_content(old_block, new_content) = {
  let d = (:)
  let fields = old_block.fields()
  fields.remove("body")
  if fields.at("below", default: none) != none {
    // TODO: this is a hack because below is a "synthesized element"
    // according to the experts in the typst discord...
    fields.below = fields.below.abs
  }
  return block.with(..fields)(new_content)
}

#let empty(v) = {
  if type(v) == str {
    // two dollar signs here because we're technically inside
    // a Pandoc template :grimace:
    v.matches(regex("^\\s*$")).at(0, default: none) != none
  } else if type(v) == content {
    if v.at("text", default: none) != none {
      return empty(v.text)
    }
    for child in v.at("children", default: ()) {
      if not empty(child) {
        return false
      }
    }
    return true
  }

}

// Subfloats
// This is a technique that we adapted from https://github.com/tingerrr/subpar/
#let quartosubfloatcounter = counter("quartosubfloatcounter")

#let quarto_super(
  kind: str,
  caption: none,
  label: none,
  supplement: str,
  position: none,
  subrefnumbering: "1a",
  subcapnumbering: "(a)",
  body,
) = {
  context {
    let figcounter = counter(figure.where(kind: kind))
    let n-super = figcounter.get().first() + 1
    set figure.caption(position: position)
    [#figure(
      kind: kind,
      supplement: supplement,
      caption: caption,
      {
        show figure.where(kind: kind): set figure(numbering: _ => numbering(subrefnumbering, n-super, quartosubfloatcounter.get().first() + 1))
        show figure.where(kind: kind): set figure.caption(position: position)

        show figure: it => {
          let num = numbering(subcapnumbering, n-super, quartosubfloatcounter.get().first() + 1)
          show figure.caption: it => {
            num.slice(2) // I don't understand why the numbering contains output that it really shouldn't, but this fixes it shrug?
            [ ]
            it.body
          }

          quartosubfloatcounter.step()
          it
          counter(figure.where(kind: it.kind)).update(n => n - 1)
        }

        quartosubfloatcounter.update(0)
        body
      }
    )#label]
  }
}

// callout rendering
// this is a figure show rule because callouts are crossreferenceable
#show figure: it => {
  if type(it.kind) != str {
    return it
  }
  let kind_match = it.kind.matches(regex("^quarto-callout-(.*)")).at(0, default: none)
  if kind_match == none {
    return it
  }
  let kind = kind_match.captures.at(0, default: "other")
  kind = upper(kind.first()) + kind.slice(1)
  // now we pull apart the callout and reassemble it with the crossref name and counter

  // when we cleanup pandoc's emitted code to avoid spaces this will have to change
  let old_callout = it.body.children.at(1).body.children.at(1)
  let old_title_block = old_callout.body.children.at(0)
  let old_title = old_title_block.body.body.children.at(2)

  // TODO use custom separator if available
  let new_title = if empty(old_title) {
    [#kind #it.counter.display()]
  } else {
    [#kind #it.counter.display(): #old_title]
  }

  let new_title_block = block_with_new_content(
    old_title_block, 
    block_with_new_content(
      old_title_block.body, 
      old_title_block.body.body.children.at(0) +
      old_title_block.body.body.children.at(1) +
      new_title))

  block_with_new_content(old_callout,
    block(below: 0pt, new_title_block) +
    old_callout.body.children.at(1))
}

// 2023-10-09: #fa-icon("fa-info") is not working, so we'll eval "#fa-info()" instead
#let callout(body: [], title: "Callout", background_color: rgb("#dddddd"), icon: none, icon_color: black, body_background_color: white) = {
  block(
    breakable: false, 
    fill: background_color, 
    stroke: (paint: icon_color, thickness: 0.5pt, cap: "round"), 
    width: 100%, 
    radius: 2pt,
    block(
      inset: 1pt,
      width: 100%, 
      below: 0pt, 
      block(
        fill: background_color, 
        width: 100%, 
        inset: 8pt)[#text(icon_color, weight: 900)[#icon] #title]) +
      if(body != []){
        block(
          inset: 1pt, 
          width: 100%, 
          block(fill: body_background_color, width: 100%, inset: 8pt, body))
      }
    )
}



#let article(
  title: none,
  subtitle: none,
  authors: none,
  date: none,
  abstract: none,
  abstract-title: none,
  cols: 1,
  lang: "en",
  region: "US",
  font: "libertinus serif",
  fontsize: 11pt,
  title-size: 1.5em,
  subtitle-size: 1.25em,
  heading-family: "libertinus serif",
  heading-weight: "bold",
  heading-style: "normal",
  heading-color: black,
  heading-line-height: 0.65em,
  sectionnumbering: none,
  toc: false,
  toc_title: none,
  toc_depth: none,
  toc_indent: 1.5em,
  doc,
) = {
  set par(justify: true)
  set text(lang: lang,
           region: region,
           font: font,
           size: fontsize)
  set heading(numbering: sectionnumbering)
  if title != none {
    align(center)[#block(inset: 2em)[
      #set par(leading: heading-line-height)
      #if (heading-family != none or heading-weight != "bold" or heading-style != "normal"
           or heading-color != black) {
        set text(font: heading-family, weight: heading-weight, style: heading-style, fill: heading-color)
        text(size: title-size)[#title]
        if subtitle != none {
          parbreak()
          text(size: subtitle-size)[#subtitle]
        }
      } else {
        text(weight: "bold", size: title-size)[#title]
        if subtitle != none {
          parbreak()
          text(weight: "bold", size: subtitle-size)[#subtitle]
        }
      }
    ]]
  }

  if authors != none {
    let count = authors.len()
    let ncols = calc.min(count, 3)
    grid(
      columns: (1fr,) * ncols,
      row-gutter: 1.5em,
      ..authors.map(author =>
          align(center)[
            #author.name \
            #author.affiliation \
            #author.email
          ]
      )
    )
  }

  if date != none {
    align(center)[#block(inset: 1em)[
      #date
    ]]
  }

  if abstract != none {
    block(inset: 2em)[
    #text(weight: "semibold")[#abstract-title] #h(1em) #abstract
    ]
  }

  if toc {
    let title = if toc_title == none {
      auto
    } else {
      toc_title
    }
    block(above: 0em, below: 2em)[
    #outline(
      title: toc_title,
      depth: toc_depth,
      indent: toc_indent
    );
    ]
  }

  if cols == 1 {
    doc
  } else {
    columns(cols, doc)
  }
}

#set table(
  inset: 6pt,
  stroke: none
)

#set page(
  paper: "us-letter",
  margin: (x: 1.25in, y: 1.25in),
  numbering: "1",
)

#show: doc => article(
  toc_title: [Table of contents],
  toc_depth: 3,
  cols: 1,
  doc,
)

= ğŸ“± UFF Instagram Analytics - Sistema RAG Inteligente
<uff-instagram-analytics---sistema-rag-inteligente>
#quote(block: true)[
Sistema de anÃ¡lise semÃ¢ntica e anÃ¡lise de sentimento para posts do Instagram dos perfis institucionais da UFF (Universidade Federal Fluminense) usando IA local.
]

#block[
#box(image("README_files/mediabag/Python-3.12--blue.svg")) #box(image("README_files/mediabag/Ollama-Local-AI-gree.svg")) #box(image("README_files/mediabag/ChromaDB-Vector-DB-o.svg")) #box(image("README_files/mediabag/Gradio-4.0--red.svg"))

#strong[#link(<-inÃ­cio-rÃ¡pido>)[InÃ­cio RÃ¡pido];] â€¢ #strong[#link(<-funcionalidades>)[Funcionalidades];] â€¢ #strong[#link(<-arquitetura>)[Arquitetura];] â€¢ #strong[#link(<-documentaÃ§Ã£o-completa>)[DocumentaÃ§Ã£o];]

]

#horizontalrule

== ğŸ“‹ Ãndice
<Ã­ndice>
- #link(<-visÃ£o-geral>)[VisÃ£o Geral]
- #link(<-funcionalidades>)[Funcionalidades]
- #link(<-arquitetura-do-sistema>)[Arquitetura do Sistema]
- #link(<-inÃ­cio-rÃ¡pido>)[InÃ­cio RÃ¡pido]
- #link(<-configuraÃ§Ã£o>)[ConfiguraÃ§Ã£o]
- #link(<-uso-da-interface>)[Uso da Interface]
- #link(<-ferramentas-disponÃ­veis>)[Ferramentas DisponÃ­veis]
- #link(<-api-rest>)[API REST]
- #link(<-documentaÃ§Ã£o-completa>)[DocumentaÃ§Ã£o Completa]
- #link(<-soluÃ§Ã£o-de-problemas>)[SoluÃ§Ã£o de Problemas]

#horizontalrule

== ğŸ¯ VisÃ£o Geral
<visÃ£o-geral>
O #strong[UFF Instagram Analytics] Ã© um sistema completo de anÃ¡lise de posts do Instagram que combina:

- ğŸ¤– #strong[Agente Inteligente] - LLM decide automaticamente quais ferramentas usar
- ğŸ” #strong[Busca SemÃ¢ntica] - Encontre posts por significado, nÃ£o apenas palavras-chave
- ğŸ“Š #strong[AnÃ¡lise Quantitativa] - EstatÃ­sticas de engajamento, ranking, comparaÃ§Ãµes
- ğŸ­ #strong[AnÃ¡lise de Sentimento] - Compreenda percepÃ§Ãµes e opiniÃµes automaticamente
- ğŸ’¬ #strong[Interface de Chat] - Pergunte em linguagem natural
- ğŸŒ #strong[100% Local] - Privacidade total, sem enviar dados para APIs externas

=== Base de Dados Atual
<base-de-dados-atual>
- #strong[2.413 posts] indexados
- #strong[3 perfis] oficiais da UFF:
  - `@dceuff` (DiretÃ³rio Central dos Estudantes) - 1.503 posts
  - `@reitor` (Reitoria da UFF) - 575 posts
  - `@vicereitor` (Vice-Reitoria da UFF) - 335 posts

#horizontalrule

== âœ¨ Funcionalidades
<funcionalidades>
=== ğŸ¯ Sistema de Agente Inteligente
<sistema-de-agente-inteligente>
O sistema usa um #strong[agente autÃ´nomo] que: 1. ğŸ“‹ #strong[Analisa] sua pergunta em linguagem natural 2. ğŸ§  #strong[Decide] automaticamente quais ferramentas usar 3. âš™ï¸ #strong[Executa] as ferramentas necessÃ¡rias (uma ou mÃºltiplas) 4. ğŸ¨ #strong[Sintetiza] uma resposta clara e completa

#strong[Exemplo:]

```
VocÃª: "Como o reitor Ã© visto pelos estudantes?"

Agente:
  1. Detecta: pergunta de sentimento
  2. Usa: analyze_sentiment(topic="reitor", profile="dceuff")
  3. Retorna: AnÃ¡lise completa com positivo/negativo, crÃ­ticas, elogios
```

=== ğŸ› ï¸ 9 Ferramentas Especializadas
<ferramentas-especializadas>
#table(
  columns: (10.71%, 39.29%, 17.86%, 32.14%),
  align: (auto,auto,auto,auto,),
  table.header([\#], [Ferramenta], [Uso], [Exemplo],),
  table.hline(),
  [1], [`get_top_posts_by_likes`], [Posts mais curtidos], ["Post mais curtido do reitor"],
  [2], [`get_top_posts_by_comments`], [Posts mais comentados], ["Top 5 com mais comentÃ¡rios"],
  [3], [`get_posts_by_engagement`], [Maior engajamento total], ["Posts com maior interaÃ§Ã£o"],
  [4], [`get_recent_posts`], [PublicaÃ§Ãµes recentes], ["Posts dos Ãºltimos 7 dias"],
  [5], [`get_profile_statistics`], [EstatÃ­sticas agregadas], ["EstatÃ­sticas do DCE"],
  [6], [`compare_profiles`], [ComparaÃ§Ã£o entre perfis], ["Compare os 3 perfis"],
  [7], [`count_term_occurrences`], [Contagem de menÃ§Ãµes], ["Quantos posts falam de greve?"],
  [8], [`analyze_sentiment`], [AnÃ¡lise de sentimento (IA)], ["Como o HUAP Ã© visto?"],
  [9], [`semantic_search`], [Busca por conteÃºdo], ["Posts sobre saÃºde"],
)
=== ğŸ­ AnÃ¡lise de Sentimento com IA
<anÃ¡lise-de-sentimento-com-ia>
Ferramenta Ãºnica que usa LLM para analisar percepÃ§Ã£o e opiniÃµes:

```
Entrada: "Como o reitor Ã© visto pelos estudantes?"

SaÃ­da:
  âœ… 5 posts positivos (25%)
  âŒ 12 posts negativos (60%)
  âšª 3 posts neutros (15%)
  
  Aspectos Positivos:
  - GestÃ£o transparente
  - DiÃ¡logo com comunidade
  
  Aspectos Negativos:
  - Demora em decisÃµes
  - Falta de comunicaÃ§Ã£o clara
  
  + Resumo narrativo completo
  + Exemplos de posts de cada categoria
```

#horizontalrule

== ğŸ—ï¸ Arquitetura do Sistema
<arquitetura-do-sistema>
=== VisÃ£o Geral
<visÃ£o-geral-1>
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERFACE GRADIO                          â”‚
â”‚              (Chat + Filtros + VisualizaÃ§Ãµes)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTE RAG (LLM)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Planejamentoâ”‚â†’ â”‚  ExecuÃ§Ã£o    â”‚â†’ â”‚  SÃ­ntese     â”‚        â”‚
â”‚  â”‚  (qwen3:30b)â”‚  â”‚ (Ferramentas)â”‚  â”‚  (qwen3:30b) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB       â”‚ â”‚ AnÃ¡lise  â”‚ â”‚ EstatÃ­sticas â”‚
â”‚ (Embeddings)   â”‚ â”‚Sentimentoâ”‚ â”‚  Agregadas   â”‚
â”‚ 2.413 posts    â”‚ â”‚  (LLM)   â”‚ â”‚   (Python)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DADOS BRUTOS (JSON)                 â”‚
â”‚  dceuff.json | reitor.json | vicereitor.jsonâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

=== Componentes Principais
<componentes-principais>
==== 1. #strong[Interface (app.py)]
<interface-app.py>
- Interface web com Gradio 4.0+
- Chat interativo com histÃ³rico
- Filtros por perfil
- ExibiÃ§Ã£o de resultados (cards, grÃ¡ficos, estatÃ­sticas)
- Avatar customizado do agente

==== 2. #strong[Agente RAG (agent\_system.py)]
<agente-rag-agent_system.py>
- #strong[Planejamento];: LLM analisa a pergunta e decide quais ferramentas usar
- #strong[ExecuÃ§Ã£o];: Roda as ferramentas escolhidas (pode ser mÃºltiplas)
- #strong[SÃ­ntese];: LLM combina resultados em resposta coerente

==== 3. #strong[Ferramentas (query\_tools.py)]
<ferramentas-query_tools.py>
- 9 ferramentas especializadas
- Queries estruturadas no ChromaDB
- AnÃ¡lise de sentimento com LLM
- EstatÃ­sticas calculadas em Python

==== 4. #strong[Embeddings (embedding\_manager.py)]
<embeddings-embedding_manager.py>
- Gerencia ChromaDB
- Modelo: `mxbai-embed-large` (669MB)
- Busca vetorial semÃ¢ntica
- PersistÃªncia em disco

==== 5. #strong[Dados (data\_loader.py)]
<dados-data_loader.py>
- Carrega posts de arquivos JSON
- Processa e limpa dados
- Extrai metadados (curtidas, comentÃ¡rios, data, etc.)

=== Fluxo de Uma Consulta
<fluxo-de-uma-consulta>
```mermaid
graph TD
    A[UsuÃ¡rio faz pergunta] --> B[Agente: Planejamento]
    B --> C{Qual ferramenta?}
    C -->|Sentimento| D[analyze_sentiment]
    C -->|Contagem| E[count_term_occurrences]
    C -->|Ranking| F[get_top_posts]
    C -->|Busca| G[semantic_search]
    D --> H[Agente: SÃ­ntese]
    E --> H
    F --> H
    G --> H
    H --> I[Resposta formatada]
    I --> J[Interface: ExibiÃ§Ã£o]
```

#horizontalrule

== ğŸš€ InÃ­cio RÃ¡pido
<inÃ­cio-rÃ¡pido>
=== PrÃ©-requisitos
<prÃ©-requisitos>
- #strong[Python 3.12+]
- #strong[uv] (gerenciador de pacotes)
- #strong[Ollama] (para rodar LLMs localmente)
- #strong[8GB RAM] mÃ­nimo (16GB recomendado)
- #strong[20GB] de espaÃ§o em disco para modelos

=== InstalaÃ§Ã£o em 3 Passos
<instalaÃ§Ã£o-em-3-passos>
==== 1. Instalar Ollama
<instalar-ollama>
```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Baixe de https://ollama.com/download
```

==== 2. Configurar Projeto
<configurar-projeto>
```bash
# Navegue atÃ© o diretÃ³rio
cd /home/marcus/projects/ping

# Sincronize dependÃªncias
uv sync
```

==== 3. Instalar Modelos
<instalar-modelos>
```bash
# Modelo de embeddings (OBRIGATÃ“RIO) - 669MB
ollama pull mxbai-embed-large

# Modelo de geraÃ§Ã£o (escolha um):

# OpÃ§Ã£o 1: Leve - 2GB RAM
ollama pull qwen2.5:3b

# OpÃ§Ã£o 2: Balanceado - 7GB RAM (recomendado)
ollama pull qwen2.5:7b

# OpÃ§Ã£o 3: Melhor qualidade - 18GB RAM
ollama pull qwen3:30b  # â† Modelo atual do sistema
```

=== Iniciar a AplicaÃ§Ã£o
<iniciar-a-aplicaÃ§Ã£o>
```bash
# Modo padrÃ£o (porta 7860)
uv run python app.py

# Com modelo especÃ­fico
uv run python app.py --generation-model qwen2.5:7b

# Criar link pÃºblico
uv run python app.py --share

# Porta customizada
uv run python app.py --port 8080
```

Acesse: #strong[http:\/\/localhost:7860]

#horizontalrule

== âš™ï¸ ConfiguraÃ§Ã£o
<configuraÃ§Ã£o>
=== Argumentos de Linha de Comando
<argumentos-de-linha-de-comando>
```bash
--embedding-model TEXT    # Modelo para embeddings
                         # PadrÃ£o: mxbai-embed-large

--generation-model TEXT   # Modelo para geraÃ§Ã£o de respostas
                         # PadrÃ£o: qwen3:30b

--port INTEGER           # Porta da aplicaÃ§Ã£o web
                         # PadrÃ£o: 7860

--share                  # Criar link pÃºblico Gradio
                         # PadrÃ£o: False
```

=== Modelos Recomendados por Recurso
<modelos-recomendados-por-recurso>
#table(
  columns: 5,
  align: (auto,auto,auto,auto,auto,),
  table.header([RAM DisponÃ­vel], [Embedding], [Generation], [Qualidade], [Velocidade],),
  table.hline(),
  [8GB], [mxbai-embed-large], [qwen2.5:3b], [â­â­], [âš¡âš¡âš¡],
  [16GB], [mxbai-embed-large], [qwen2.5:7b], [â­â­â­], [âš¡âš¡],
  [32GB+], [mxbai-embed-large], [qwen3:30b], [â­â­â­â­â­], [âš¡],
)
=== Estrutura de Dados (JSON)
<estrutura-de-dados-json>
Os posts devem estar em `data/` no formato:

```json
[
  {
    "id": "3737403160894992541",
    "type": "Video",
    "caption": "Texto da legenda do post...",
    "hashtags": ["uff", "universidade"],
    "mentions": ["@perfil"],
    "url": "https://www.instagram.com/p/ABC123/",
    "commentsCount": 7,
    "likesCount": 124,
    "timestamp": "2025-10-06T14:58:54.000Z",
    "latestComments": [
      {
        "text": "Ã“tima iniciativa!",
        "ownerUsername": "usuario123"
      }
    ]
  }
]
```

#horizontalrule

== ğŸ’¬ Uso da Interface
<uso-da-interface>
=== Painel Principal
<painel-principal>
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“± UFF Instagram Analytics                             â”‚
â”‚  FaÃ§a perguntas sobre os 2.413 posts                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ’¬ CHAT                     â”‚  âš™ï¸ CONFIGURAÃ‡Ã•ES        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  Filtrar por Perfil:     â”‚
â”‚  â”‚ Bot: OlÃ¡!        â”‚        â”‚  [ğŸŒ Todos os Perfis â–¼] â”‚
â”‚  â”‚ User: Quantos... â”‚        â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  ğŸ“Š EstatÃ­sticas         â”‚
â”‚                              â”‚  ğŸ’¡ Exemplos             â”‚
â”‚  [Digite sua pergunta...]    â”‚  ğŸ† Post mais curtido    â”‚
â”‚  [Enviar ğŸš€]                 â”‚  ğŸ“Š Compare perfis       â”‚
â”‚                              â”‚  ğŸ” Posts sobre HUAP     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

=== Exemplos de Perguntas
<exemplos-de-perguntas>
==== ğŸ“Š AnÃ¡lise Quantitativa
<anÃ¡lise-quantitativa>
```
âœ… "Quantos posts falam sobre greve?"
â†’ Usa: count_term_occurrences
â†’ Retorna: 42 posts (1.74%)

âœ… "Qual foi o post mais curtido do reitor?"
â†’ Usa: get_top_posts_by_likes(profile="reitor", limit=1)
â†’ Retorna: Post com 1.234 curtidas + link

âœ… "Compare o engajamento dos 3 perfis"
â†’ Usa: compare_profiles()
â†’ Retorna: Tabela comparativa completa
```

==== ğŸ” Busca SemÃ¢ntica
<busca-semÃ¢ntica>
```
âœ… "Posts sobre saÃºde e hospital"
â†’ Usa: semantic_search(query="saÃºde hospital HUAP atendimento")
â†’ Retorna: 10 posts mais relevantes

âœ… "O que foi dito sobre a greve em 2024?"
â†’ Usa: semantic_search + filtro temporal
â†’ Retorna: Posts relevantes ordenados

âœ… "Ãšltima apariÃ§Ã£o pÃºblica do reitor"
â†’ Usa: semantic_search(profile="reitor") + get_recent_posts
â†’ Retorna: Post mais recente relevante
```

==== ğŸ­ AnÃ¡lise de Sentimento
<anÃ¡lise-de-sentimento>
```
âœ… "Como o reitor Ã© visto pelos estudantes?"
â†’ Usa: analyze_sentiment(topic="reitor", profile="dceuff")
â†’ Retorna:
  â€¢ 60% negativos, 25% positivos, 15% neutros
  â€¢ Aspectos positivos: transparÃªncia, diÃ¡logo
  â€¢ CrÃ­ticas: demora, falta de comunicaÃ§Ã£o
  â€¢ Resumo narrativo + exemplos

âœ… "Qual a percepÃ§Ã£o sobre o HUAP?"
â†’ Usa: analyze_sentiment(topic="HUAP")
â†’ Retorna: AnÃ¡lise completa de sentimento

âœ… "O que pensam sobre a gestÃ£o?"
â†’ Usa: analyze_sentiment(topic="gestÃ£o")
â†’ Retorna: OpiniÃµes e tendÃªncias identificadas
```

==== ğŸ“ˆ EstatÃ­sticas
<estatÃ­sticas>
```
âœ… "EstatÃ­sticas do DCE"
â†’ Usa: get_profile_statistics(profile="dceuff")
â†’ Retorna:
  â€¢ 1.503 posts
  â€¢ 45.678 curtidas totais
  â€¢ MÃ©dia: 30.4 curtidas/post
  â€¢ Post mais engajado

âœ… "Posts da Ãºltima semana"
â†’ Usa: get_recent_posts(days=7)
â†’ Retorna: Todos os posts recentes

âœ… "Top 5 posts com mais comentÃ¡rios"
â†’ Usa: get_top_posts_by_comments(limit=5)
â†’ Retorna: Ranking com links
```

#horizontalrule

== ğŸ› ï¸ Ferramentas DisponÃ­veis
<ferramentas-disponÃ­veis>
=== 1. get\_top\_posts\_by\_likes
<get_top_posts_by_likes>
#strong[Uso:] Encontrar posts mais curtidos \
#strong[ParÃ¢metros:] - `limit` (int): Quantidade de posts - `profile` (str, opcional): Filtrar por perfil

#strong[Exemplo:]

```python
tools.get_top_posts_by_likes(limit=10, profile="reitor")
```

=== 2. get\_top\_posts\_by\_comments
<get_top_posts_by_comments>
#strong[Uso:] Posts com mais comentÃ¡rios \
#strong[ParÃ¢metros:] - `limit` (int): Quantidade - `profile` (str, opcional): Perfil

=== 3. get\_posts\_by\_engagement
<get_posts_by_engagement>
#strong[Uso:] Maior engajamento (curtidas + comentÃ¡rios) \
#strong[ParÃ¢metros:] - `limit` (int): Quantidade - `profile` (str, opcional): Perfil

=== 4. get\_recent\_posts
<get_recent_posts>
#strong[Uso:] PublicaÃ§Ãµes recentes \
#strong[ParÃ¢metros:] - `days` (int): Ãšltimos N dias - `limit` (int): Quantidade - `profile` (str, opcional): Perfil

=== 5. get\_profile\_statistics
<get_profile_statistics>
#strong[Uso:] EstatÃ­sticas agregadas de um perfil \
#strong[ParÃ¢metros:] - `profile` (str, opcional): Se vazio, retorna todos

#strong[Retorna:]

```json
{
  "total_posts": 1503,
  "total_likes": 45678,
  "total_comments": 2341,
  "avg_likes_per_post": 30.4,
  "avg_comments_per_post": 1.6,
  "total_engagement": 48019,
  "top_post": {...}
}
```

=== 6. compare\_profiles
<compare_profiles>
#strong[Uso:] Comparar todos os perfis \
#strong[Sem parÃ¢metros]

#strong[Retorna:]

```json
{
  "dceuff": {
    "total_posts": 1503,
    "total_likes": 45678,
    "avg_likes": 30.4,
    ...
  },
  "reitor": {...},
  "vicereitor": {...}
}
```

=== 7. count\_term\_occurrences â­ NOVO
<count_term_occurrences-novo>
#strong[Uso:] Quantificar menÃ§Ãµes de um termo \
#strong[ParÃ¢metros:] - `term` (str): Termo a buscar - `profile` (str, opcional): Perfil - `case_sensitive` (bool): MaiÃºsculas/minÃºsculas

#strong[Retorna:]

```json
{
  "count": 42,
  "percentage": 1.74,
  "total_posts": 2413,
  "term": "greve",
  "matching_posts": [...]
}
```

#strong[DiferenÃ§a de semantic\_search:] - `count_term_occurrences`: #strong[QUANTIFICA] (todos os posts) - `semantic_search`: #strong[QUALIFICA] (posts mais relevantes)

=== 8. analyze\_sentiment â­ NOVO - IA
<analyze_sentiment-novo---ia>
#strong[Uso:] AnÃ¡lise de sentimento com LLM \
#strong[ParÃ¢metros:] - `topic` (str): TÃ³pico/entidade - `profile` (str, opcional): Perfil - `n_posts` (int): Posts a analisar (padrÃ£o: 20)

#strong[Retorna:]

```json
{
  "topic": "reitor",
  "sentiment_summary": "AnÃ¡lise narrativa...",
  "positive_count": 5,
  "negative_count": 12,
  "neutral_count": 3,
  "positive_aspects": ["transparÃªncia", "diÃ¡logo"],
  "negative_aspects": ["demora", "comunicaÃ§Ã£o"],
  "key_points": [...],
  "examples": {
    "positive": [...],
    "negative": [...],
    "neutral": [...]
  }
}
```

#strong[Como funciona:] 1. Busca posts que mencionam o tÃ³pico 2. Seleciona atÃ© N posts para anÃ¡lise 3. LLM analisa e classifica cada post 4. Extrai aspectos positivos e negativos 5. Gera resumo qualitativo 6. Retorna estatÃ­sticas + exemplos

=== 9. semantic\_search
<semantic_search>
#strong[Uso:] Busca vetorial por conteÃºdo \
#strong[ParÃ¢metros:] - `query` (str): Consulta semÃ¢ntica - `n_results` (int): Quantidade - `profile` (str, opcional): Perfil

#strong[Como funciona:] - Converte query em embedding - Busca posts similares no espaÃ§o vetorial - Retorna os N mais relevantes

#horizontalrule

== ğŸŒ API REST
<api-rest>
A aplicaÃ§Ã£o Gradio expÃµe uma API REST automÃ¡tica.

=== Endpoint Principal
<endpoint-principal>
```
POST http://localhost:7860/api/predict
```

=== Fazer uma Pergunta
<fazer-uma-pergunta>
```bash
curl -X POST http://localhost:7860/api/predict \
  -H "Content-Type: application/json" \
  -d '{
    "data": [
      "Quantos posts falam de greve?",
      [],
      5,
      "ğŸŒ Todos os Perfis"
    ]
  }'
```

#strong[ParÃ¢metros (array `data`):] 1. Pergunta (string) 2. HistÃ³rico do chat (array, pode ser `[]`) 3. NÃºmero de resultados (int, ignorado no modo agente) 4. Filtro de perfil (string: "ğŸŒ Todos os Perfis", "#cite(<dceuff>, form: "prose");", "#cite(<reitor>, form: "prose");", "#cite(<vicereitor>, form: "prose");")

=== Resposta
<resposta>
```json
{
  "data": [
    "",  // Input vazio (limpo apÃ³s envio)
    [    // HistÃ³rico atualizado
      [
        "Quantos posts falam de greve?",
        "Encontrados 42 posts (1.74%) que mencionam 'greve'..."
      ]
    ],
    "<div>...</div>"  // HTML dos posts recuperados
  ],
  "duration": 2.34
}
```

=== Exemplo com Python
<exemplo-com-python>
```python
import requests

response = requests.post(
    "http://localhost:7860/api/predict",
    json={
        "data": [
            "Como o reitor Ã© visto?",
            [],
            5,
            "@dceuff"
        ]
    }
)

result = response.json()
answer = result['data'][1][0][1]  # Resposta do bot
print(answer)
```

#horizontalrule

== ğŸ“š DocumentaÃ§Ã£o Completa
<documentaÃ§Ã£o-completa>
=== Arquivos de DocumentaÃ§Ã£o
<arquivos-de-documentaÃ§Ã£o>
Toda a documentaÃ§Ã£o estÃ¡ consolidada aqui, mas arquivos individuais ainda existem:

#table(
  columns: 2,
  align: (auto,auto,),
  table.header([Arquivo], [ConteÃºdo],),
  table.hline(),
  [`README.md`], [#strong[Este arquivo] - DocumentaÃ§Ã£o completa],
  [`QUICKSTART.md`], [Guia rÃ¡pido de inÃ­cio],
  [`API_QUICKSTART.md`], [Exemplos de uso da API],
  [`TOOLS.md`], [Detalhes de todas as ferramentas],
  [`SENTIMENT_ANALYSIS_TOOL.md`], [AnÃ¡lise de sentimento (ferramenta \#8)],
  [`TERM_COUNT_TOOL.md`], [Contagem de termos (ferramenta \#7)],
  [`ARCHITECTURE.md`], [Arquitetura detalhada],
  [`AGENT_VS_CLASSIC.md`], [ComparaÃ§Ã£o agente vs sistema clÃ¡ssico],
  [`BALANCED_AGENT.md`], [Como o agente equilibra ferramentas],
)
=== Estrutura de Arquivos
<estrutura-de-arquivos>
```
ping/
â”œâ”€â”€ ğŸ“ data/                      # Dados dos posts (JSON)
â”‚   â”œâ”€â”€ dceuff.json              # 1.503 posts
â”‚   â”œâ”€â”€ reitor.json              # 575 posts
â”‚   â””â”€â”€ vicereitor.json          # 335 posts
â”‚
â”œâ”€â”€ ğŸ“ chroma_db/                # Banco vetorial (auto-gerado)
â”‚   â””â”€â”€ ...                      # Embeddings persistidos
â”‚
â”œâ”€â”€ ğŸ“ assets/                   # Assets da interface
â”‚   â””â”€â”€ agent_avatar.png         # Avatar do agente
â”‚
â”œâ”€â”€ ğŸ CÃ“DIGO PRINCIPAL
â”‚   â”œâ”€â”€ app.py                   # Interface Gradio
â”‚   â”œâ”€â”€ agent_system.py          # Sistema de agente RAG
â”‚   â”œâ”€â”€ query_tools.py           # 9 ferramentas especializadas
â”‚   â”œâ”€â”€ embedding_manager.py     # Gerenciador ChromaDB
â”‚   â”œâ”€â”€ data_loader.py           # Carregador de dados
â”‚   â””â”€â”€ rag_system.py            # Sistema RAG clÃ¡ssico (legado)
â”‚
â”œâ”€â”€ ğŸ“„ CONFIGURAÃ‡ÃƒO
â”‚   â”œâ”€â”€ pyproject.toml           # DependÃªncias (uv)
â”‚   â””â”€â”€ .python-version          # Python 3.12+
â”‚
â”œâ”€â”€ ğŸ§ª TESTES
â”‚   â”œâ”€â”€ test_term_count.py       # Teste de contagem
â”‚   â””â”€â”€ check_profiles.py        # Debug de perfis
â”‚
â””â”€â”€ ğŸ“š DOCUMENTAÃ‡ÃƒO
    â”œâ”€â”€ README.md                # â† VOCÃŠ ESTÃ AQUI
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ API_QUICKSTART.md
    â”œâ”€â”€ TOOLS.md
    â”œâ”€â”€ SENTIMENT_ANALYSIS_TOOL.md
    â”œâ”€â”€ TERM_COUNT_TOOL.md
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ AGENT_VS_CLASSIC.md
    â””â”€â”€ BALANCED_AGENT.md
```

=== Tecnologias Utilizadas
<tecnologias-utilizadas>
#table(
  columns: 3,
  align: (auto,auto,auto,),
  table.header([Tecnologia], [VersÃ£o], [Uso],),
  table.hline(),
  [#strong[Python];], [3.12+], [Linguagem principal],
  [#strong[uv];], [Latest], [Gerenciador de pacotes],
  [#strong[Ollama];], [Latest], [Runtime para LLMs locais],
  [#strong[ChromaDB];], [Latest], [Banco de dados vetorial],
  [#strong[Gradio];], [4.0+], [Interface web],
  [#strong[mxbai-embed-large];], [669MB], [Modelo de embeddings],
  [#strong[qwen3:30b];], [18GB], [Modelo de geraÃ§Ã£o (padrÃ£o)],
)

#horizontalrule

== ğŸ› SoluÃ§Ã£o de Problemas
<soluÃ§Ã£o-de-problemas>
=== Erro: "Model not found"
<erro-model-not-found>
```bash
# Verifique modelos instalados
ollama list

# Instale o modelo necessÃ¡rio
ollama pull mxbai-embed-large
ollama pull qwen3:30b
```

=== Erro: "Connection refused" (Ollama)
<erro-connection-refused-ollama>
```bash
# Verifique se Ollama estÃ¡ rodando
ollama list

# Se nÃ£o estiver, inicie:
ollama serve

# Ou no sistema
ps aux | grep ollama
```

=== ChromaDB nÃ£o persiste dados
<chromadb-nÃ£o-persiste-dados>
```bash
# Verifique permissÃµes
chmod -R 755 chroma_db/

# Ou recrie do zero
rm -rf chroma_db/
uv run python app.py  # ReindexarÃ¡ automaticamente
```

=== Interface nÃ£o mostra todos os perfis
<interface-nÃ£o-mostra-todos-os-perfis>
```bash
# Verifique se os 3 perfis estÃ£o carregados
uv run python check_profiles.py

# Deve mostrar:
# Perfis encontrados: ['dceuff', 'reitor', 'vicereitor']

# Limpe cache do navegador (Ctrl+Shift+R)
```

=== MemÃ³ria insuficiente
<memÃ³ria-insuficiente>
#strong[OpÃ§Ã£o 1: Use modelo menor]

```bash
uv run python app.py --generation-model qwen2.5:3b
```

#strong[OpÃ§Ã£o 2: Libere memÃ³ria] - Feche outros aplicativos - Reinicie Ollama: `killall ollama && ollama serve`

#strong[OpÃ§Ã£o 3: Configure swap (Linux)]

```bash
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

=== Respostas lentas
<respostas-lentas>
+ #strong[Use modelo mais leve:]

  ```bash
  uv run python app.py --generation-model qwen2.5:7b
  ```

+ #strong[Reduza posts analisados:]

  - AnÃ¡lise de sentimento: max 20 posts
  - Busca semÃ¢ntica: max 10 posts

+ #strong[Verifique GPU:]

  ```bash
  # Se tiver NVIDIA GPU
  nvidia-smi
  # Ollama usa GPU automaticamente
  ```

=== Erro de parsing JSON (anÃ¡lise de sentimento)
<erro-de-parsing-json-anÃ¡lise-de-sentimento>
Ã€s vezes o LLM retorna JSON malformado. O sistema tem fallback automÃ¡tico, mas vocÃª pode:

+ Usar modelo maior (qwen3:30b mais confiÃ¡vel que qwen2.5:3b)
+ Reduzir nÃºmero de posts analisados
+ Tentar novamente (LLMs podem variar)

#horizontalrule

== ğŸ“Š Performance e Benchmarks
<performance-e-benchmarks>
=== Tempos MÃ©dios (Hardware: 16GB RAM, qwen3:30b)
<tempos-mÃ©dios-hardware-16gb-ram-qwen330b>
#table(
  columns: 4,
  align: (auto,auto,auto,auto,),
  table.header([OperaÃ§Ã£o], [Quantidade], [Tempo], [Cache],),
  table.hline(),
  [IndexaÃ§Ã£o inicial], [2.413 posts], [\~8 min], [N/A],
  [Busca semÃ¢ntica], [10 resultados], [\~1-2s], [Warm],
  [Contagem de termo], [Toda base], [\~1-2s], [N/A],
  [AnÃ¡lise de sentimento], [20 posts], [\~8-15s], [N/A],
  [EstatÃ­sticas], [1 perfil], [\~0.5s], [N/A],
  [SÃ­ntese LLM], [1 resposta], [\~3-8s], [Warm],
)
=== Uso de Recursos
<uso-de-recursos>
#table(
  columns: 4,
  align: (auto,auto,auto,auto,),
  table.header([Componente], [RAM], [Disco], [GPU],),
  table.hline(),
  [ChromaDB], [\~200MB], [\~50MB], [NÃ£o],
  [mxbai-embed-large], [\~700MB], [669MB], [Opcional],
  [qwen3:30b], [\~18GB], [18GB], [Sim\*],
  [Gradio], [\~100MB], [-], [NÃ£o],
  [#strong[Total];], [#strong[\~19GB];], [#strong[\~19GB];], [#strong[Opcional];],
)
\*GPU acelera significativamente (3-5x mais rÃ¡pido)

#horizontalrule

== ğŸ¤ Contribuindo
<contribuindo>
Melhorias sÃ£o bem-vindas! Ãreas de interesse:

=== Funcionalidades
<funcionalidades-1>
- â˜ AnÃ¡lise temporal (trends ao longo do tempo)
- â˜ ExportaÃ§Ã£o de relatÃ³rios (PDF, CSV)
- â˜ VisualizaÃ§Ãµes (grÃ¡ficos, word clouds)
- â˜ Suporte a mais redes sociais
- â˜ Sistema de cache inteligente
- â˜ AnÃ¡lise de imagens (multimodal)

=== Melhorias TÃ©cnicas
<melhorias-tÃ©cnicas>
- â˜ Testes automatizados
- â˜ CI/CD pipeline
- â˜ Docker container
- â˜ DocumentaÃ§Ã£o em inglÃªs
- â˜ Logs estruturados
- â˜ Monitoring e mÃ©tricas

=== Como Contribuir
<como-contribuir>
+ Fork o repositÃ³rio
+ Crie uma branch (`git checkout -b feature/MinhaFeature`)
+ Commit suas mudanÃ§as (`git commit -m 'Add: MinhaFeature'`)
+ Push para a branch (`git push origin feature/MinhaFeature`)
+ Abra um Pull Request

#horizontalrule

== ğŸ“„ LicenÃ§a
<licenÃ§a>
Este projeto Ã© de cÃ³digo aberto para fins educacionais e institucionais.

#horizontalrule

== ğŸ™ CrÃ©ditos
<crÃ©ditos>
=== Tecnologias
<tecnologias>
- #strong[#link("https://ollama.com/")[Ollama];] - Runtime para LLMs locais
- #strong[#link("https://www.trychroma.com/")[ChromaDB];] - Banco de dados vetorial
- #strong[#link("https://gradio.app/")[Gradio];] - Framework de interface web
- #strong[#link("https://docs.astral.sh/uv/")[uv];] - Gerenciador de pacotes Python moderno

=== Modelos de IA
<modelos-de-ia>
- #strong[mxbai-embed-large] - Embeddings (mixedbread.ai)
- #strong[qwen3:30b / qwen2.5] - Modelos de linguagem (Alibaba Cloud)

=== Desenvolvimento
<desenvolvimento>
Desenvolvido para anÃ¡lise de comunicaÃ§Ã£o institucional da #strong[Universidade Federal Fluminense (UFF)];.

#horizontalrule

== ğŸ“ Suporte
<suporte>
- #strong[Issues:] Abra uma issue no repositÃ³rio
- #strong[DocumentaÃ§Ã£o:] Consulte os arquivos `.md` na raiz
- #strong[Comunidade:] Compartilhe sua experiÃªncia e melhorias

#horizontalrule

#block[
#strong[#link(<-uff-instagram-analytics---sistema-rag-inteligente>)[â¬† Voltar ao topo];]

#horizontalrule

Feito com â¤ï¸ para a comunidade UFF \
#emph[VersÃ£o 2.0 - Outubro 2025]

]




