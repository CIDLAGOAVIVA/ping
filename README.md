# ğŸ“± Instagram RAG - Sistema de AnÃ¡lise de Posts Institucionais

Sistema RAG (Retrieval-Augmented Generation) para anÃ¡lise semÃ¢ntica de posts do Instagram dos perfis institucionais da UFF (Universidade Federal Fluminense).

## ğŸ¯ CaracterÃ­sticas

- **Busca SemÃ¢ntica**: Encontre posts relevantes usando linguagem natural
- **AnÃ¡lise Contextual**: Respostas baseadas em evidÃªncias reais dos posts
- **Interface AmigÃ¡vel**: Chat interativo com Gradio
- **100% Local**: ExecuÃ§Ã£o completa em sua mÃ¡quina usando Ollama
- **MÃºltiplos Perfis**: Analise posts de diferentes perfis institucionais simultaneamente

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UsuÃ¡rio   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Pergunta
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Interface Gradio  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Sistema RAG      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embeddings   â”‚  â”‚
â”‚  â”‚  (ChromaDB)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GeraÃ§Ã£o LLM  â”‚  â”‚
â”‚  â”‚   (Ollama)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Posts JSON        â”‚
â”‚   data/             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ PrÃ©-requisitos

1. **Python 3.12+**
2. **uv** (gerenciador de pacotes Python)
3. **Ollama** instalado e rodando
4. MÃ­nimo 8GB RAM (16GB recomendado)
5. ~20GB espaÃ§o em disco para modelos

## ğŸš€ InstalaÃ§Ã£o

### 1. Instalar Ollama

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Baixe de https://ollama.com/download
```

### 2. Clonar e configurar o projeto

```bash
# Clone o repositÃ³rio (ou navegue atÃ© o diretÃ³rio)
cd ping

# Sincronizar dependÃªncias com uv
uv sync
```

### 3. Instalar modelos do Ollama

Execute o script de setup para instalar os modelos necessÃ¡rios:

```bash
# Modelo de embedding (obrigatÃ³rio)
ollama pull mxbai-embed-large

# Modelo de geraÃ§Ã£o - escolha um:

# OpÃ§Ã£o 1: Leve e rÃ¡pido (~2GB)
ollama pull qwen2.5:3b

# OpÃ§Ã£o 2: Melhor qualidade (~7GB)
ollama pull qwen2.5:7b

# OpÃ§Ã£o 3: MÃ¡xima qualidade (~18GB)
ollama pull qwen2.5:14b

# Alternativas:
# ollama pull gpt-oss:20b  # 13GB
# ollama pull gpt-oss:120b # 65GB (requer muito recurso)
```

## ğŸ“Š Estrutura de Dados

Os arquivos JSON devem estar na pasta `data/` com a seguinte estrutura:

```json
[
  {
    "id": "3737403160894992541",
    "type": "Video",
    "caption": "Texto da legenda...",
    "hashtags": ["tag1", "tag2"],
    "mentions": ["perfil1"],
    "url": "https://www.instagram.com/p/ABC123/",
    "commentsCount": 7,
    "likesCount": 124,
    "timestamp": "2025-10-06T14:58:54.000Z",
    "latestComments": [
      {
        "text": "ComentÃ¡rio...",
        "ownerUsername": "usuario"
      }
    ]
  }
]
```

## ğŸ® Uso

### Iniciar a aplicaÃ§Ã£o

```bash
# Com uv
uv run python app.py

# Com argumentos personalizados
uv run python app.py --generation-model qwen2.5:7b --port 8080

# Criar link pÃºblico (share)
uv run python app.py --share
```

### OpÃ§Ãµes de linha de comando

```bash
--embedding-model    # Modelo para embeddings (padrÃ£o: mxbai-embed-large)
--generation-model   # Modelo para geraÃ§Ã£o (padrÃ£o: qwen2.5:3b)
--port              # Porta da aplicaÃ§Ã£o (padrÃ£o: 7860)
--share             # Criar link pÃºblico do Gradio
```

### Acessar a interface

ApÃ³s iniciar, abra seu navegador em:
```
http://localhost:7860
```

## ğŸ’¡ Exemplos de Perguntas

### Busca Simples
- "Quais foram os posts mais recentes do DCE UFF?"
- "Mostre posts sobre o HUAP"
- "O que foi publicado sobre pesquisa?"

### AnÃ¡lise de Engajamento
- "Quais posts tiveram mais curtidas?"
- "Posts com mais comentÃ¡rios nos Ãºltimos 30 dias"
- "Compare o engajamento entre os perfis"

### Busca por Tema
- "Posts que mencionam estudantes"
- "PublicaÃ§Ãµes sobre eventos"
- "Novidades sobre infraestrutura"

### Filtros Temporais
- "Posts de setembro de 2024"
- "Ãšltimas publicaÃ§Ãµes do reitor"
- "ConteÃºdo recente do vice-reitor"

## ğŸ”§ Testes Individuais

### Testar carregamento de dados
```bash
uv run python data_loader.py
```

### Testar embeddings
```bash
uv run python embedding_manager.py
```

### Testar sistema RAG completo
```bash
uv run python rag_system.py
```

## ğŸ“ Estrutura do Projeto

```
ping/
â”œâ”€â”€ data/                    # Arquivos JSON dos posts
â”‚   â”œâ”€â”€ dceuff.json
â”‚   â”œâ”€â”€ reitor.json
â”‚   â””â”€â”€ vicereitor.json
â”œâ”€â”€ chroma_db/              # Banco vetorial (gerado automaticamente)
â”œâ”€â”€ app.py                  # AplicaÃ§Ã£o Gradio principal
â”œâ”€â”€ rag_system.py           # Sistema RAG completo
â”œâ”€â”€ embedding_manager.py    # Gerenciamento de embeddings
â”œâ”€â”€ data_loader.py          # Carregamento e processamento de dados
â”œâ”€â”€ pyproject.toml          # ConfiguraÃ§Ã£o do projeto
â””â”€â”€ README.md              # Esta documentaÃ§Ã£o
```

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### Modelos Recomendados por Recurso

| Recurso DisponÃ­vel | Embedding Model | Generation Model | Qualidade |
|-------------------|----------------|------------------|-----------|
| 8GB RAM | mxbai-embed-large | qwen2.5:3b | BÃ¡sica |
| 16GB RAM | mxbai-embed-large | qwen2.5:7b | Boa |
| 32GB+ RAM | mxbai-embed-large | qwen2.5:14b | Excelente |

### Alternativas de Modelos de Embedding

```bash
# OpÃ§Ã£o 1: RÃ¡pido e eficiente (padrÃ£o)
ollama pull mxbai-embed-large

# OpÃ§Ã£o 2: Alta qualidade
ollama pull nomic-embed-text

# OpÃ§Ã£o 3: MultilÃ­ngue
ollama pull snowflake-arctic-embed
```

### Reindexar Posts

Se vocÃª adicionar novos arquivos JSON ou quiser reindexar:

```python
from rag_system import RAGSystem

rag = RAGSystem()
rag.index_all_posts(force_reindex=True)
```

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Model not found"
```bash
# Certifique-se de que o Ollama estÃ¡ rodando
ollama list

# Instale o modelo necessÃ¡rio
ollama pull <nome-do-modelo>
```

### Erro: "Connection refused" (Ollama)
```bash
# Inicie o serviÃ§o Ollama
# Linux/macOS
ollama serve

# Ou verifique se estÃ¡ rodando
ps aux | grep ollama
```

### ChromaDB nÃ£o persiste dados
```bash
# Verifique permissÃµes do diretÃ³rio
chmod -R 755 chroma_db/

# Ou remova e recrie
rm -rf chroma_db/
uv run python rag_system.py
```

### MemÃ³ria insuficiente
- Use modelo menor: `qwen2.5:3b` em vez de `qwen2.5:14b`
- Reduza `batch_size` em `embedding_manager.py`
- Feche outros aplicativos

## ğŸ“ˆ Performance

### Tempos Esperados (aprox.)

| OperaÃ§Ã£o | Quantidade | Tempo |
|----------|-----------|-------|
| IndexaÃ§Ã£o inicial | 1000 posts | ~5-10 min |
| Busca semÃ¢ntica | 5 resultados | ~1-2 seg |
| GeraÃ§Ã£o resposta | 1 resposta | ~3-10 seg |

*Tempos variam conforme hardware e modelo escolhido*

## ğŸ¤ Contribuindo

Melhorias sÃ£o bem-vindas! Ãreas de interesse:

- Suporte a mais formatos de dados
- Filtros temporais avanÃ§ados
- VisualizaÃ§Ãµes de dados
- AnÃ¡lise de sentimento
- ExportaÃ§Ã£o de relatÃ³rios

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto. Use livremente para fins educacionais e institucionais.

## ğŸ™ Agradecimentos

ConstruÃ­do com:
- [Ollama](https://ollama.com/) - Modelos de linguagem locais
- [ChromaDB](https://www.trychroma.com/) - Banco de dados vetorial
- [Gradio](https://gradio.app/) - Interface web interativa
- [uv](https://docs.astral.sh/uv/) - Gerenciador de pacotes Python

---

**Desenvolvido para anÃ¡lise de posts institucionais da UFF**

Para dÃºvidas ou sugestÃµes, abra uma issue no repositÃ³rio.
