# ğŸ“± UFF Instagram Analytics - Sistema RAG Inteligente

> Sistema de anÃ¡lise semÃ¢ntica e anÃ¡lise de sentimento para posts do Instagram dos perfis institucionais da UFF (Universidade Federal Fluminense) usando IA local.

<div align="center">

![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)
![Ollama](https://img.shields.io/badge/Ollama-Local%20AI-green.svg)
![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20DB-orange.svg)
![Gradio](https://img.shields.io/badge/Gradio-4.0+-red.svg)

**[InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido)** â€¢ **[Funcionalidades](#-funcionalidades)** â€¢ **[Arquitetura](#-arquitetura)** â€¢ **[DocumentaÃ§Ã£o](#-documentaÃ§Ã£o-completa)**

</div>

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Funcionalidades](#-funcionalidades)
- [Arquitetura do Sistema](#-arquitetura-do-sistema)
- [InÃ­cio RÃ¡pido](#-inÃ­cio-rÃ¡pido)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o)
- [Uso da Interface](#-uso-da-interface)
- [Ferramentas DisponÃ­veis](#-ferramentas-disponÃ­veis)
- [API REST](#-api-rest)
- [DocumentaÃ§Ã£o Completa](#-documentaÃ§Ã£o-completa)
- [SoluÃ§Ã£o de Problemas](#-soluÃ§Ã£o-de-problemas)

---

## ğŸ¯ VisÃ£o Geral

O **UFF Instagram Analytics** Ã© um sistema completo de anÃ¡lise de posts do Instagram que combina:

- ğŸ¤– **Agente Inteligente** - LLM decide automaticamente quais ferramentas usar
- ğŸ” **Busca SemÃ¢ntica** - Encontre posts por significado, nÃ£o apenas palavras-chave
- ğŸ“Š **AnÃ¡lise Quantitativa** - EstatÃ­sticas de engajamento, ranking, comparaÃ§Ãµes
- ğŸ­ **AnÃ¡lise de Sentimento** - Compreenda percepÃ§Ãµes e opiniÃµes automaticamente
- ğŸ’¬ **Interface de Chat** - Pergunte em linguagem natural
- ğŸŒ **100% Local** - Privacidade total, sem enviar dados para APIs externas

### Base de Dados Atual

- **2.413 posts** indexados
- **3 perfis** oficiais da UFF:
  - `@dceuff` (DiretÃ³rio Central dos Estudantes) - 1.503 posts
  - `@reitor` (Reitoria da UFF) - 575 posts
  - `@vicereitor` (Vice-Reitoria da UFF) - 335 posts

---

## âœ¨ Funcionalidades

### ğŸ¯ Sistema de Agente Inteligente

O sistema usa um **agente autÃ´nomo** que:
1. ğŸ“‹ **Analisa** sua pergunta em linguagem natural
2. ğŸ§  **Decide** automaticamente quais ferramentas usar
3. âš™ï¸ **Executa** as ferramentas necessÃ¡rias (uma ou mÃºltiplas)
4. ğŸ¨ **Sintetiza** uma resposta clara e completa

**Exemplo:** 
```
VocÃª: "Como o reitor Ã© visto pelos estudantes?"

Agente:
  1. Detecta: pergunta de sentimento
  2. Usa: analyze_sentiment(topic="reitor", profile="dceuff")
  3. Retorna: AnÃ¡lise completa com positivo/negativo, crÃ­ticas, elogios
```

### ğŸ› ï¸ 9 Ferramentas Especializadas

| # | Ferramenta | Uso | Exemplo |
|---|-----------|-----|---------|
| 1 | `get_top_posts_by_likes` | Posts mais curtidos | "Post mais curtido do reitor" |
| 2 | `get_top_posts_by_comments` | Posts mais comentados | "Top 5 com mais comentÃ¡rios" |
| 3 | `get_posts_by_engagement` | Maior engajamento total | "Posts com maior interaÃ§Ã£o" |
| 4 | `get_recent_posts` | PublicaÃ§Ãµes recentes | "Posts dos Ãºltimos 7 dias" |
| 5 | `get_profile_statistics` | EstatÃ­sticas agregadas | "EstatÃ­sticas do DCE" |
| 6 | `compare_profiles` | ComparaÃ§Ã£o entre perfis | "Compare os 3 perfis" |
| 7 | `count_term_occurrences` | Contagem de menÃ§Ãµes | "Quantos posts falam de greve?" |
| 8 | `analyze_sentiment` | AnÃ¡lise de sentimento (IA) | "Como o HUAP Ã© visto?" |
| 9 | `semantic_search` | Busca por conteÃºdo | "Posts sobre saÃºde" |

### ğŸ­ AnÃ¡lise de Sentimento com IA

Ferramenta Ãºnica que usa LLM para analisar percepÃ§Ã£o e opiniÃµes:

```
Entrada: "Como o reitor Ã© visto pelos estudantes?"

SaÃ­da:
  âœ… 5 posts positivos (25%)
  âŒ 12 posts negativos (60%)
  âšª 3 posts neutros (15%)
  
  Aspectos Positivos:
  - GestÃ£o transparente
  - DiÃ¡logo com comunidade
  
  Aspectos Negativos:
  - Demora em decisÃµes
  - Falta de comunicaÃ§Ã£o clara
  
  + Resumo narrativo completo
  + Exemplos de posts de cada categoria
```

---

## ğŸ—ï¸ Arquitetura do Sistema

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERFACE GRADIO                          â”‚
â”‚              (Chat + Filtros + VisualizaÃ§Ãµes)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENTE RAG (LLM)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Planejamentoâ”‚â†’ â”‚  ExecuÃ§Ã£o    â”‚â†’ â”‚  SÃ­ntese     â”‚        â”‚
â”‚  â”‚  (qwen3:30b)â”‚  â”‚ (Ferramentas)â”‚  â”‚  (qwen3:30b) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚
         â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB       â”‚ â”‚ AnÃ¡lise  â”‚ â”‚ EstatÃ­sticas â”‚
â”‚ (Embeddings)   â”‚ â”‚Sentimentoâ”‚ â”‚  Agregadas   â”‚
â”‚ 2.413 posts    â”‚ â”‚  (LLM)   â”‚ â”‚   (Python)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DADOS BRUTOS (JSON)                 â”‚
â”‚  dceuff.json | reitor.json | vicereitor.jsonâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

#### 1. **Interface (app.py)**
- Interface web com Gradio 4.0+
- Chat interativo com histÃ³rico
- Filtros por perfil
- ExibiÃ§Ã£o de resultados (cards, grÃ¡ficos, estatÃ­sticas)
- Avatar customizado do agente

#### 2. **Agente RAG (agent_system.py)**
- **Planejamento**: LLM analisa a pergunta e decide quais ferramentas usar
- **ExecuÃ§Ã£o**: Roda as ferramentas escolhidas (pode ser mÃºltiplas)
- **SÃ­ntese**: LLM combina resultados em resposta coerente

#### 3. **Ferramentas (query_tools.py)**
- 9 ferramentas especializadas
- Queries estruturadas no ChromaDB
- AnÃ¡lise de sentimento com LLM
- EstatÃ­sticas calculadas em Python

#### 4. **Embeddings (embedding_manager.py)**
- Gerencia ChromaDB
- Modelo: `mxbai-embed-large` (669MB)
- Busca vetorial semÃ¢ntica
- PersistÃªncia em disco

#### 5. **Dados (data_loader.py)**
- Carrega posts de arquivos JSON
- Processa e limpa dados
- Extrai metadados (curtidas, comentÃ¡rios, data, etc.)

### Fluxo de Uma Consulta

```mermaid
graph TD
    A[UsuÃ¡rio faz pergunta] --> B[Agente: Planejamento]
    B --> C{Qual ferramenta?}
    C -->|Sentimento| D[analyze_sentiment]
    C -->|Contagem| E[count_term_occurrences]
    C -->|Ranking| F[get_top_posts]
    C -->|Busca| G[semantic_search]
    D --> H[Agente: SÃ­ntese]
    E --> H
    F --> H
    G --> H
    H --> I[Resposta formatada]
    I --> J[Interface: ExibiÃ§Ã£o]
```

---

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

- **Python 3.12+**
- **uv** (gerenciador de pacotes)
- **Ollama** (para rodar LLMs localmente)
- **8GB RAM** mÃ­nimo (16GB recomendado)
- **20GB** de espaÃ§o em disco para modelos

### InstalaÃ§Ã£o em 3 Passos

#### 1. Instalar Ollama

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Baixe de https://ollama.com/download
```

#### 2. Configurar Projeto

```bash
# Navegue atÃ© o diretÃ³rio
cd /home/marcus/projects/ping

# Sincronize dependÃªncias
uv sync
```

#### 3. Instalar Modelos

```bash
# Modelo de embeddings (OBRIGATÃ“RIO) - 669MB
ollama pull mxbai-embed-large

# Modelo de geraÃ§Ã£o (escolha um):

# OpÃ§Ã£o 1: Leve - 2GB RAM
ollama pull qwen2.5:3b

# OpÃ§Ã£o 2: Balanceado - 7GB RAM (recomendado)
ollama pull qwen2.5:7b

# OpÃ§Ã£o 3: Melhor qualidade - 18GB RAM
ollama pull qwen3:30b  # â† Modelo atual do sistema
```

### Iniciar a AplicaÃ§Ã£o

```bash
# Modo padrÃ£o (porta 7860)
uv run python app.py

# Com modelo especÃ­fico
uv run python app.py --generation-model qwen2.5:7b

# Criar link pÃºblico
uv run python app.py --share

# Porta customizada
uv run python app.py --port 8080
```

Acesse: **http://localhost:7860**

---

## âš™ï¸ ConfiguraÃ§Ã£o

### Argumentos de Linha de Comando

```bash
--embedding-model TEXT    # Modelo para embeddings
                         # PadrÃ£o: mxbai-embed-large

--generation-model TEXT   # Modelo para geraÃ§Ã£o de respostas
                         # PadrÃ£o: qwen3:30b

--port INTEGER           # Porta da aplicaÃ§Ã£o web
                         # PadrÃ£o: 7860

--share                  # Criar link pÃºblico Gradio
                         # PadrÃ£o: False
```

### Modelos Recomendados por Recurso

| RAM DisponÃ­vel | Embedding | Generation | Qualidade | Velocidade |
|---------------|-----------|------------|-----------|------------|
| 8GB  | mxbai-embed-large | qwen2.5:3b | â­â­ | âš¡âš¡âš¡ |
| 16GB | mxbai-embed-large | qwen2.5:7b | â­â­â­ | âš¡âš¡ |
| 32GB+ | mxbai-embed-large | qwen3:30b | â­â­â­â­â­ | âš¡ |

### Estrutura de Dados (JSON)

Os posts devem estar em `data/` no formato:

```json
[
  {
    "id": "3737403160894992541",
    "type": "Video",
    "caption": "Texto da legenda do post...",
    "hashtags": ["uff", "universidade"],
    "mentions": ["@perfil"],
    "url": "https://www.instagram.com/p/ABC123/",
    "commentsCount": 7,
    "likesCount": 124,
    "timestamp": "2025-10-06T14:58:54.000Z",
    "latestComments": [
      {
        "text": "Ã“tima iniciativa!",
        "ownerUsername": "usuario123"
      }
    ]
  }
]
```

---

## ğŸ’¬ Uso da Interface

### Painel Principal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“± UFF Instagram Analytics                             â”‚
â”‚  FaÃ§a perguntas sobre os 2.413 posts                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ’¬ CHAT                     â”‚  âš™ï¸ CONFIGURAÃ‡Ã•ES        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  Filtrar por Perfil:     â”‚
â”‚  â”‚ Bot: OlÃ¡!        â”‚        â”‚  [ğŸŒ Todos os Perfis â–¼] â”‚
â”‚  â”‚ User: Quantos... â”‚        â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  ğŸ“Š EstatÃ­sticas         â”‚
â”‚                              â”‚  ğŸ’¡ Exemplos             â”‚
â”‚  [Digite sua pergunta...]    â”‚  ğŸ† Post mais curtido    â”‚
â”‚  [Enviar ğŸš€]                 â”‚  ğŸ“Š Compare perfis       â”‚
â”‚                              â”‚  ğŸ” Posts sobre HUAP     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemplos de Perguntas

#### ğŸ“Š AnÃ¡lise Quantitativa

```
âœ… "Quantos posts falam sobre greve?"
â†’ Usa: count_term_occurrences
â†’ Retorna: 42 posts (1.74%)

âœ… "Qual foi o post mais curtido do reitor?"
â†’ Usa: get_top_posts_by_likes(profile="reitor", limit=1)
â†’ Retorna: Post com 1.234 curtidas + link

âœ… "Compare o engajamento dos 3 perfis"
â†’ Usa: compare_profiles()
â†’ Retorna: Tabela comparativa completa
```

#### ğŸ” Busca SemÃ¢ntica

```
âœ… "Posts sobre saÃºde e hospital"
â†’ Usa: semantic_search(query="saÃºde hospital HUAP atendimento")
â†’ Retorna: 10 posts mais relevantes

âœ… "O que foi dito sobre a greve em 2024?"
â†’ Usa: semantic_search + filtro temporal
â†’ Retorna: Posts relevantes ordenados

âœ… "Ãšltima apariÃ§Ã£o pÃºblica do reitor"
â†’ Usa: semantic_search(profile="reitor") + get_recent_posts
â†’ Retorna: Post mais recente relevante
```

#### ğŸ­ AnÃ¡lise de Sentimento

```
âœ… "Como o reitor Ã© visto pelos estudantes?"
â†’ Usa: analyze_sentiment(topic="reitor", profile="dceuff")
â†’ Retorna:
  â€¢ 60% negativos, 25% positivos, 15% neutros
  â€¢ Aspectos positivos: transparÃªncia, diÃ¡logo
  â€¢ CrÃ­ticas: demora, falta de comunicaÃ§Ã£o
  â€¢ Resumo narrativo + exemplos

âœ… "Qual a percepÃ§Ã£o sobre o HUAP?"
â†’ Usa: analyze_sentiment(topic="HUAP")
â†’ Retorna: AnÃ¡lise completa de sentimento

âœ… "O que pensam sobre a gestÃ£o?"
â†’ Usa: analyze_sentiment(topic="gestÃ£o")
â†’ Retorna: OpiniÃµes e tendÃªncias identificadas
```

#### ğŸ“ˆ EstatÃ­sticas

```
âœ… "EstatÃ­sticas do DCE"
â†’ Usa: get_profile_statistics(profile="dceuff")
â†’ Retorna:
  â€¢ 1.503 posts
  â€¢ 45.678 curtidas totais
  â€¢ MÃ©dia: 30.4 curtidas/post
  â€¢ Post mais engajado

âœ… "Posts da Ãºltima semana"
â†’ Usa: get_recent_posts(days=7)
â†’ Retorna: Todos os posts recentes

âœ… "Top 5 posts com mais comentÃ¡rios"
â†’ Usa: get_top_posts_by_comments(limit=5)
â†’ Retorna: Ranking com links
```

---

## ğŸ› ï¸ Ferramentas DisponÃ­veis

### 1. get_top_posts_by_likes
**Uso:** Encontrar posts mais curtidos  
**ParÃ¢metros:**
- `limit` (int): Quantidade de posts
- `profile` (str, opcional): Filtrar por perfil

**Exemplo:**
```python
tools.get_top_posts_by_likes(limit=10, profile="reitor")
```

### 2. get_top_posts_by_comments
**Uso:** Posts com mais comentÃ¡rios  
**ParÃ¢metros:**
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 3. get_posts_by_engagement
**Uso:** Maior engajamento (curtidas + comentÃ¡rios)  
**ParÃ¢metros:**
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 4. get_recent_posts
**Uso:** PublicaÃ§Ãµes recentes  
**ParÃ¢metros:**
- `days` (int): Ãšltimos N dias
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 5. get_profile_statistics
**Uso:** EstatÃ­sticas agregadas de um perfil  
**ParÃ¢metros:**
- `profile` (str, opcional): Se vazio, retorna todos

**Retorna:**
```json
{
  "total_posts": 1503,
  "total_likes": 45678,
  "total_comments": 2341,
  "avg_likes_per_post": 30.4,
  "avg_comments_per_post": 1.6,
  "total_engagement": 48019,
  "top_post": {...}
}
```

### 6. compare_profiles
**Uso:** Comparar todos os perfis  
**Sem parÃ¢metros**

**Retorna:**
```json
{
  "dceuff": {
    "total_posts": 1503,
    "total_likes": 45678,
    "avg_likes": 30.4,
    ...
  },
  "reitor": {...},
  "vicereitor": {...}
}
```

### 7. count_term_occurrences â­ NOVO
**Uso:** Quantificar menÃ§Ãµes de um termo  
**ParÃ¢metros:**
- `term` (str): Termo a buscar
- `profile` (str, opcional): Perfil
- `case_sensitive` (bool): MaiÃºsculas/minÃºsculas

**Retorna:**
```json
{
  "count": 42,
  "percentage": 1.74,
  "total_posts": 2413,
  "term": "greve",
  "matching_posts": [...]
}
```

**DiferenÃ§a de semantic_search:**
- `count_term_occurrences`: **QUANTIFICA** (todos os posts)
- `semantic_search`: **QUALIFICA** (posts mais relevantes)

### 8. analyze_sentiment â­ NOVO - IA
**Uso:** AnÃ¡lise de sentimento com LLM  
**ParÃ¢metros:**
- `topic` (str): TÃ³pico/entidade
- `profile` (str, opcional): Perfil
- `n_posts` (int): Posts a analisar (padrÃ£o: 20)

**Retorna:**
```json
{
  "topic": "reitor",
  "sentiment_summary": "AnÃ¡lise narrativa...",
  "positive_count": 5,
  "negative_count": 12,
  "neutral_count": 3,
  "positive_aspects": ["transparÃªncia", "diÃ¡logo"],
  "negative_aspects": ["demora", "comunicaÃ§Ã£o"],
  "key_points": [...],
  "examples": {
    "positive": [...],
    "negative": [...],
    "neutral": [...]
  }
}
```

**Como funciona:**
1. Busca posts que mencionam o tÃ³pico
2. Seleciona atÃ© N posts para anÃ¡lise
3. LLM analisa e classifica cada post
4. Extrai aspectos positivos e negativos
5. Gera resumo qualitativo
6. Retorna estatÃ­sticas + exemplos

### 9. semantic_search
**Uso:** Busca vetorial por conteÃºdo  
**ParÃ¢metros:**
- `query` (str): Consulta semÃ¢ntica
- `n_results` (int): Quantidade
- `profile` (str, opcional): Perfil

**Como funciona:**
- Converte query em embedding
- Busca posts similares no espaÃ§o vetorial
- Retorna os N mais relevantes

---

## ğŸŒ API REST

A aplicaÃ§Ã£o Gradio expÃµe uma API REST automÃ¡tica.

### Endpoint Principal

```
POST http://localhost:7860/api/predict
```

### Fazer uma Pergunta

```bash
curl -X POST http://localhost:7860/api/predict \
  -H "Content-Type: application/json" \
  -d '{
    "data": [
      "Quantos posts falam de greve?",
      [],
      5,
      "ğŸŒ Todos os Perfis"
    ]
  }'
```

**ParÃ¢metros (array `data`):**
1. Pergunta (string)
2. HistÃ³rico do chat (array, pode ser `[]`)
3. NÃºmero de resultados (int, ignorado no modo agente)
4. Filtro de perfil (string: "ğŸŒ Todos os Perfis", "@dceuff", "@reitor", "@vicereitor")

### Resposta

```json
{
  "data": [
    "",  // Input vazio (limpo apÃ³s envio)
    [    // HistÃ³rico atualizado
      [
        "Quantos posts falam de greve?",
        "Encontrados 42 posts (1.74%) que mencionam 'greve'..."
      ]
    ],
    "<div>...</div>"  // HTML dos posts recuperados
  ],
  "duration": 2.34
}
```

### Exemplo com Python

```python
import requests

response = requests.post(
    "http://localhost:7860/api/predict",
    json={
        "data": [
            "Como o reitor Ã© visto?",
            [],
            5,
            "@dceuff"
        ]
    }
)

result = response.json()
answer = result['data'][1][0][1]  # Resposta do bot
print(answer)
```

---

## ğŸ“š DocumentaÃ§Ã£o Completa

### Arquivos de DocumentaÃ§Ã£o

Toda a documentaÃ§Ã£o estÃ¡ consolidada aqui, mas arquivos individuais ainda existem:

| Arquivo | ConteÃºdo |
|---------|----------|
| `README.md` | **Este arquivo** - DocumentaÃ§Ã£o completa |
| `QUICKSTART.md` | Guia rÃ¡pido de inÃ­cio |
| `API_QUICKSTART.md` | Exemplos de uso da API |
| `TOOLS.md` | Detalhes de todas as ferramentas |
| `SENTIMENT_ANALYSIS_TOOL.md` | AnÃ¡lise de sentimento (ferramenta #8) |
| `TERM_COUNT_TOOL.md` | Contagem de termos (ferramenta #7) |
| `ARCHITECTURE.md` | Arquitetura detalhada |
| `AGENT_VS_CLASSIC.md` | ComparaÃ§Ã£o agente vs sistema clÃ¡ssico |
| `BALANCED_AGENT.md` | Como o agente equilibra ferramentas |

### Estrutura de Arquivos

```
ping/
â”œâ”€â”€ ğŸ“ data/                      # Dados dos posts (JSON)
â”‚   â”œâ”€â”€ dceuff.json              # 1.503 posts
â”‚   â”œâ”€â”€ reitor.json              # 575 posts
â”‚   â””â”€â”€ vicereitor.json          # 335 posts
â”‚
â”œâ”€â”€ ğŸ“ chroma_db/                # Banco vetorial (auto-gerado)
â”‚   â””â”€â”€ ...                      # Embeddings persistidos
â”‚
â”œâ”€â”€ ğŸ“ assets/                   # Assets da interface
â”‚   â””â”€â”€ agent_avatar.png         # Avatar do agente
â”‚
â”œâ”€â”€ ğŸ CÃ“DIGO PRINCIPAL
â”‚   â”œâ”€â”€ app.py                   # Interface Gradio
â”‚   â”œâ”€â”€ agent_system.py          # Sistema de agente RAG
â”‚   â”œâ”€â”€ query_tools.py           # 9 ferramentas especializadas
â”‚   â”œâ”€â”€ embedding_manager.py     # Gerenciador ChromaDB
â”‚   â”œâ”€â”€ data_loader.py           # Carregador de dados
â”‚   â””â”€â”€ rag_system.py            # Sistema RAG clÃ¡ssico (legado)
â”‚
â”œâ”€â”€ ğŸ“„ CONFIGURAÃ‡ÃƒO
â”‚   â”œâ”€â”€ pyproject.toml           # DependÃªncias (uv)
â”‚   â””â”€â”€ .python-version          # Python 3.12+
â”‚
â”œâ”€â”€ ğŸ§ª TESTES
â”‚   â”œâ”€â”€ test_term_count.py       # Teste de contagem
â”‚   â””â”€â”€ check_profiles.py        # Debug de perfis
â”‚
â””â”€â”€ ğŸ“š DOCUMENTAÃ‡ÃƒO
    â”œâ”€â”€ README.md                # â† VOCÃŠ ESTÃ AQUI
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ API_QUICKSTART.md
    â”œâ”€â”€ TOOLS.md
    â”œâ”€â”€ SENTIMENT_ANALYSIS_TOOL.md
    â”œâ”€â”€ TERM_COUNT_TOOL.md
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ AGENT_VS_CLASSIC.md
    â””â”€â”€ BALANCED_AGENT.md
```

### Tecnologias Utilizadas

| Tecnologia | VersÃ£o | Uso |
|-----------|--------|-----|
| **Python** | 3.12+ | Linguagem principal |
| **uv** | Latest | Gerenciador de pacotes |
| **Ollama** | Latest | Runtime para LLMs locais |
| **ChromaDB** | Latest | Banco de dados vetorial |
| **Gradio** | 4.0+ | Interface web |
| **mxbai-embed-large** | 669MB | Modelo de embeddings |
| **qwen3:30b** | 18GB | Modelo de geraÃ§Ã£o (padrÃ£o) |

---

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Model not found"

```bash
# Verifique modelos instalados
ollama list

# Instale o modelo necessÃ¡rio
ollama pull mxbai-embed-large
ollama pull qwen3:30b
```

### Erro: "Connection refused" (Ollama)

```bash
# Verifique se Ollama estÃ¡ rodando
ollama list

# Se nÃ£o estiver, inicie:
ollama serve

# Ou no sistema
ps aux | grep ollama
```

### ChromaDB nÃ£o persiste dados

```bash
# Verifique permissÃµes
chmod -R 755 chroma_db/

# Ou recrie do zero
rm -rf chroma_db/
uv run python app.py  # ReindexarÃ¡ automaticamente
```

### Interface nÃ£o mostra todos os perfis

```bash
# Verifique se os 3 perfis estÃ£o carregados
uv run python check_profiles.py

# Deve mostrar:
# Perfis encontrados: ['dceuff', 'reitor', 'vicereitor']

# Limpe cache do navegador (Ctrl+Shift+R)
```

### MemÃ³ria insuficiente

**OpÃ§Ã£o 1: Use modelo menor**
```bash
uv run python app.py --generation-model qwen2.5:3b
```

**OpÃ§Ã£o 2: Libere memÃ³ria**
- Feche outros aplicativos
- Reinicie Ollama: `killall ollama && ollama serve`

**OpÃ§Ã£o 3: Configure swap (Linux)**
```bash
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Respostas lentas

1. **Use modelo mais leve:**
   ```bash
   uv run python app.py --generation-model qwen2.5:7b
   ```

2. **Reduza posts analisados:**
   - AnÃ¡lise de sentimento: max 20 posts
   - Busca semÃ¢ntica: max 10 posts

3. **Verifique GPU:**
   ```bash
   # Se tiver NVIDIA GPU
   nvidia-smi
   # Ollama usa GPU automaticamente
   ```

### Erro de parsing JSON (anÃ¡lise de sentimento)

Ã€s vezes o LLM retorna JSON malformado. O sistema tem fallback automÃ¡tico, mas vocÃª pode:

1. Usar modelo maior (qwen3:30b mais confiÃ¡vel que qwen2.5:3b)
2. Reduzir nÃºmero de posts analisados
3. Tentar novamente (LLMs podem variar)

---

## ğŸ“Š Performance e Benchmarks

### Tempos MÃ©dios (Hardware: 16GB RAM, qwen3:30b)

| OperaÃ§Ã£o | Quantidade | Tempo | Cache |
|----------|-----------|-------|-------|
| IndexaÃ§Ã£o inicial | 2.413 posts | ~8 min | N/A |
| Busca semÃ¢ntica | 10 resultados | ~1-2s | Warm |
| Contagem de termo | Toda base | ~1-2s | N/A |
| AnÃ¡lise de sentimento | 20 posts | ~8-15s | N/A |
| EstatÃ­sticas | 1 perfil | ~0.5s | N/A |
| SÃ­ntese LLM | 1 resposta | ~3-8s | Warm |

### Uso de Recursos

| Componente | RAM | Disco | GPU |
|-----------|-----|-------|-----|
| ChromaDB | ~200MB | ~50MB | NÃ£o |
| mxbai-embed-large | ~700MB | 669MB | Opcional |
| qwen3:30b | ~18GB | 18GB | Sim* |
| Gradio | ~100MB | - | NÃ£o |
| **Total** | **~19GB** | **~19GB** | **Opcional** |

*GPU acelera significativamente (3-5x mais rÃ¡pido)

---

## ğŸ¤ Contribuindo

Melhorias sÃ£o bem-vindas! Ãreas de interesse:

### Funcionalidades
- [ ] AnÃ¡lise temporal (trends ao longo do tempo)
- [ ] ExportaÃ§Ã£o de relatÃ³rios (PDF, CSV)
- [ ] VisualizaÃ§Ãµes (grÃ¡ficos, word clouds)
- [ ] Suporte a mais redes sociais
- [ ] Sistema de cache inteligente
- [ ] AnÃ¡lise de imagens (multimodal)

### Melhorias TÃ©cnicas
- [ ] Testes automatizados
- [ ] CI/CD pipeline
- [ ] Docker container
- [ ] DocumentaÃ§Ã£o em inglÃªs
- [ ] Logs estruturados
- [ ] Monitoring e mÃ©tricas

### Como Contribuir

1. Fork o repositÃ³rio
2. Crie uma branch (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto para fins educacionais e institucionais.

---

## ğŸ™ CrÃ©ditos

### Tecnologias

- **[Ollama](https://ollama.com/)** - Runtime para LLMs locais
- **[ChromaDB](https://www.trychroma.com/)** - Banco de dados vetorial
- **[Gradio](https://gradio.app/)** - Framework de interface web
- **[uv](https://docs.astral.sh/uv/)** - Gerenciador de pacotes Python moderno

### Modelos de IA

- **mxbai-embed-large** - Embeddings (mixedbread.ai)
- **qwen3:30b / qwen2.5** - Modelos de linguagem (Alibaba Cloud)

### Desenvolvimento

Desenvolvido para anÃ¡lise de comunicaÃ§Ã£o institucional da **Universidade Federal Fluminense (UFF)**.

---

## ğŸ“ Suporte

- **Issues:** Abra uma issue no repositÃ³rio
- **DocumentaÃ§Ã£o:** Consulte os arquivos `.md` na raiz
- **Comunidade:** Compartilhe sua experiÃªncia e melhorias

---

<div align="center">

**[â¬† Voltar ao topo](#-uff-instagram-analytics---sistema-rag-inteligente)**

---

Feito com â¤ï¸ para a comunidade UFF  
*VersÃ£o 2.0 - Outubro 2025*

</div>
