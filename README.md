# üì± UFF Instagram Analytics - Sistema RAG Inteligente

> Sistema de an√°lise sem√¢ntica e an√°lise de sentimento para posts do Instagram dos perfis institucionais da UFF (Universidade Federal Fluminense) usando IA local.

<div align="center">

![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)
![Ollama](https://img.shields.io/badge/Ollama-Local%20AI-green.svg)
![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20DB-orange.svg)
![Gradio](https://img.shields.io/badge/Gradio-4.0+-red.svg)

**[In√≠cio R√°pido](#-in√≠cio-r√°pido)** ‚Ä¢ **[Funcionalidades](#-funcionalidades)** ‚Ä¢ **[Arquitetura](#-arquitetura)** ‚Ä¢ **[Documenta√ß√£o](#-documenta√ß√£o-completa)**

</div>

---

## üìã √çndice

- [Vis√£o Geral](#-vis√£o-geral)
- [Funcionalidades](#-funcionalidades)
- [Arquitetura do Sistema](#-arquitetura-do-sistema)
- [In√≠cio R√°pido](#-in√≠cio-r√°pido)
- [Configura√ß√£o](#-configura√ß√£o)
- [Uso da Interface](#-uso-da-interface)
- [Ferramentas Dispon√≠veis](#-ferramentas-dispon√≠veis)
- [API REST](#-api-rest)
- [Documenta√ß√£o Completa](#-documenta√ß√£o-completa)
- [Solu√ß√£o de Problemas](#-solu√ß√£o-de-problemas)

---

## üéØ Vis√£o Geral

O **UFF Instagram Analytics** √© um sistema completo de an√°lise de posts do Instagram que combina:

- ü§ñ **Agente Inteligente** - LLM decide automaticamente quais ferramentas usar
- üîç **Busca Sem√¢ntica** - Encontre posts por significado, n√£o apenas palavras-chave
- üìä **An√°lise Quantitativa** - Estat√≠sticas de engajamento, ranking, compara√ß√µes
- üé≠ **An√°lise de Sentimento** - Compreenda percep√ß√µes e opini√µes automaticamente
- üí¨ **Interface de Chat** - Pergunte em linguagem natural
- üåê **100% Local** - Privacidade total, sem enviar dados para APIs externas

### Base de Dados Atual

- **2.413 posts** indexados
- **3 perfis** oficiais da UFF:
  - `@dceuff` (Diret√≥rio Central dos Estudantes) - 1.503 posts
  - `@reitor` (Reitoria da UFF) - 575 posts
  - `@vicereitor` (Vice-Reitoria da UFF) - 335 posts

---

## ‚ú® Funcionalidades

### üéØ Sistema de Agente Inteligente

O sistema usa um **agente aut√¥nomo** que:
1. üìã **Analisa** sua pergunta em linguagem natural
2. üß† **Decide** automaticamente quais ferramentas usar
3. ‚öôÔ∏è **Executa** as ferramentas necess√°rias (uma ou m√∫ltiplas)
4. üé® **Sintetiza** uma resposta clara e completa

**Exemplo:** 
```
Voc√™: "Como o reitor √© visto pelos estudantes?"

Agente:
  1. Detecta: pergunta de sentimento
  2. Usa: analyze_sentiment(topic="reitor", profile="dceuff")
  3. Retorna: An√°lise completa com positivo/negativo, cr√≠ticas, elogios
```

### üõ†Ô∏è 9 Ferramentas Especializadas

| # | Ferramenta | Uso | Exemplo |
|---|-----------|-----|---------|
| 1 | `get_top_posts_by_likes` | Posts mais curtidos | "Post mais curtido do reitor" |
| 2 | `get_top_posts_by_comments` | Posts mais comentados | "Top 5 com mais coment√°rios" |
| 3 | `get_posts_by_engagement` | Maior engajamento total | "Posts com maior intera√ß√£o" |
| 4 | `get_recent_posts` | Publica√ß√µes recentes | "Posts dos √∫ltimos 7 dias" |
| 5 | `get_profile_statistics` | Estat√≠sticas agregadas | "Estat√≠sticas do DCE" |
| 6 | `compare_profiles` | Compara√ß√£o entre perfis | "Compare os 3 perfis" |
| 7 | `count_term_occurrences` | Contagem de men√ß√µes | "Quantos posts falam de greve?" |
| 8 | `analyze_sentiment` | An√°lise de sentimento (IA) | "Como o HUAP √© visto?" |
| 9 | `semantic_search` | Busca por conte√∫do | "Posts sobre sa√∫de" |

### üé≠ An√°lise de Sentimento com IA

Ferramenta √∫nica que usa LLM para analisar percep√ß√£o e opini√µes:

```
Entrada: "Como o reitor √© visto pelos estudantes?"

Sa√≠da:
  ‚úÖ 5 posts positivos (25%)
  ‚ùå 12 posts negativos (60%)
  ‚ö™ 3 posts neutros (15%)
  
  Aspectos Positivos:
  - Gest√£o transparente
  - Di√°logo com comunidade
  
  Aspectos Negativos:
  - Demora em decis√µes
  - Falta de comunica√ß√£o clara
  
  + Resumo narrativo completo
  + Exemplos de posts de cada categoria
```

---

## üèóÔ∏è Arquitetura do Sistema

### Vis√£o Geral

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERFACE GRADIO                          ‚îÇ
‚îÇ              (Chat + Filtros + Visualiza√ß√µes)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AGENTE RAG (LLM)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ Planejamento‚îÇ‚Üí ‚îÇ  Execu√ß√£o    ‚îÇ‚Üí ‚îÇ  S√≠ntese     ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  (qwen3:30b)‚îÇ  ‚îÇ (Ferramentas)‚îÇ  ‚îÇ  (qwen3:30b) ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚ñº               ‚ñº               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ChromaDB       ‚îÇ ‚îÇ An√°lise  ‚îÇ ‚îÇ Estat√≠sticas ‚îÇ
‚îÇ (Embeddings)   ‚îÇ ‚îÇSentimento‚îÇ ‚îÇ  Agregadas   ‚îÇ
‚îÇ 2.413 posts    ‚îÇ ‚îÇ  (LLM)   ‚îÇ ‚îÇ   (Python)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         DADOS BRUTOS (JSON)                 ‚îÇ
‚îÇ  dceuff.json | reitor.json | vicereitor.json‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Principais

#### 1. **Interface (app.py)**
- Interface web com Gradio 4.0+
- Chat interativo com hist√≥rico
- Filtros por perfil
- Exibi√ß√£o de resultados (cards, gr√°ficos, estat√≠sticas)
- Avatar customizado do agente

#### 2. **Agente RAG (agent_system.py)**
- **Planejamento**: LLM analisa a pergunta e decide quais ferramentas usar
- **Execu√ß√£o**: Roda as ferramentas escolhidas (pode ser m√∫ltiplas)
- **S√≠ntese**: LLM combina resultados em resposta coerente

#### 3. **Ferramentas (query_tools.py)**
- 9 ferramentas especializadas
- Queries estruturadas no ChromaDB
- An√°lise de sentimento com LLM
- Estat√≠sticas calculadas em Python

#### 4. **Embeddings (embedding_manager.py)**
- Gerencia ChromaDB
- Modelo: `mxbai-embed-large` (669MB)
- Busca vetorial sem√¢ntica
- Persist√™ncia em disco

#### 5. **Dados (data_loader.py)**
- Carrega posts de arquivos JSON
- Processa e limpa dados
- Extrai metadados (curtidas, coment√°rios, data, etc.)

### Fluxo de Uma Consulta

```mermaid
graph TD
    A[Usu√°rio faz pergunta] --> B[Agente: Planejamento]
    B --> C{Qual ferramenta?}
    C -->|Sentimento| D[analyze_sentiment]
    C -->|Contagem| E[count_term_occurrences]
    C -->|Ranking| F[get_top_posts]
    C -->|Busca| G[semantic_search]
    D --> H[Agente: S√≠ntese]
    E --> H
    F --> H
    G --> H
    H --> I[Resposta formatada]
    I --> J[Interface: Exibi√ß√£o]
```

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

- **Python 3.12+**
- **uv** (gerenciador de pacotes)
- **Ollama** (para rodar LLMs localmente)
- **8GB RAM** m√≠nimo (16GB recomendado)
- **20GB** de espa√ßo em disco para modelos

### Instala√ß√£o em 3 Passos

#### 1. Instalar Ollama

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Windows
# Baixe de https://ollama.com/download
```

#### 2. Configurar Projeto

```bash
# Navegue at√© o diret√≥rio
cd /home/marcus/projects/ping

# Sincronize depend√™ncias
uv sync
```

#### 3. Instalar Modelos

```bash
# Modelo de embeddings (OBRIGAT√ìRIO) - 669MB
ollama pull mxbai-embed-large

# Modelo de gera√ß√£o (escolha um):

# Op√ß√£o 1: Leve - 2GB RAM
ollama pull qwen2.5:3b

# Op√ß√£o 2: Balanceado - 7GB RAM (recomendado)
ollama pull qwen2.5:7b

# Op√ß√£o 3: Melhor qualidade - 18GB RAM
ollama pull qwen3:30b  # ‚Üê Modelo atual do sistema
```

### Iniciar a Aplica√ß√£o

```bash
# Modo padr√£o (porta 7860)
uv run python app.py

# Com modelo espec√≠fico
uv run python app.py --generation-model qwen2.5:7b

# Criar link p√∫blico
uv run python app.py --share

# Porta customizada
uv run python app.py --port 8080
```

Acesse: **http://localhost:7860**

---

## ‚öôÔ∏è Configura√ß√£o

### Argumentos de Linha de Comando

```bash
--embedding-model TEXT    # Modelo para embeddings
                         # Padr√£o: mxbai-embed-large

--generation-model TEXT   # Modelo para gera√ß√£o de respostas
                         # Padr√£o: qwen3:30b

--port INTEGER           # Porta da aplica√ß√£o web
                         # Padr√£o: 7860

--share                  # Criar link p√∫blico Gradio
                         # Padr√£o: False
```

### Modelos Recomendados por Recurso

| RAM Dispon√≠vel | Embedding | Generation | Qualidade | Velocidade |
|---------------|-----------|------------|-----------|------------|
| 8GB  | mxbai-embed-large | qwen2.5:3b | ‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| 16GB | mxbai-embed-large | qwen2.5:7b | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° |
| 32GB+ | mxbai-embed-large | qwen3:30b | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° |

### Estrutura de Dados (JSON)

Os posts devem estar em `data/` no formato:

```json
[
  {
    "id": "3737403160894992541",
    "type": "Video",
    "caption": "Texto da legenda do post...",
    "hashtags": ["uff", "universidade"],
    "mentions": ["@perfil"],
    "url": "https://www.instagram.com/p/ABC123/",
    "commentsCount": 7,
    "likesCount": 124,
    "timestamp": "2025-10-06T14:58:54.000Z",
    "latestComments": [
      {
        "text": "√ìtima iniciativa!",
        "ownerUsername": "usuario123"
      }
    ]
  }
]
```

---

## üí¨ Uso da Interface

### Painel Principal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üì± UFF Instagram Analytics                             ‚îÇ
‚îÇ  Fa√ßa perguntas sobre os 2.413 posts                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  üí¨ CHAT                     ‚îÇ  ‚öôÔ∏è CONFIGURA√á√ïES        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ  Filtrar por Perfil:     ‚îÇ
‚îÇ  ‚îÇ Bot: Ol√°!        ‚îÇ        ‚îÇ  [üåê Todos os Perfis ‚ñº] ‚îÇ
‚îÇ  ‚îÇ User: Quantos... ‚îÇ        ‚îÇ                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ  üìä Estat√≠sticas         ‚îÇ
‚îÇ                              ‚îÇ  üí° Exemplos             ‚îÇ
‚îÇ  [Digite sua pergunta...]    ‚îÇ  üèÜ Post mais curtido    ‚îÇ
‚îÇ  [Enviar üöÄ]                 ‚îÇ  üìä Compare perfis       ‚îÇ
‚îÇ                              ‚îÇ  üîç Posts sobre HUAP     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Exemplos de Perguntas

#### üìä An√°lise Quantitativa

```
‚úÖ "Quantos posts falam sobre greve?"
‚Üí Usa: count_term_occurrences
‚Üí Retorna: 42 posts (1.74%)

‚úÖ "Qual foi o post mais curtido do reitor?"
‚Üí Usa: get_top_posts_by_likes(profile="reitor", limit=1)
‚Üí Retorna: Post com 1.234 curtidas + link

‚úÖ "Compare o engajamento dos 3 perfis"
‚Üí Usa: compare_profiles()
‚Üí Retorna: Tabela comparativa completa
```

#### üîç Busca Sem√¢ntica

```
‚úÖ "Posts sobre sa√∫de e hospital"
‚Üí Usa: semantic_search(query="sa√∫de hospital HUAP atendimento")
‚Üí Retorna: 10 posts mais relevantes

‚úÖ "O que foi dito sobre a greve em 2024?"
‚Üí Usa: semantic_search + filtro temporal
‚Üí Retorna: Posts relevantes ordenados

‚úÖ "√öltima apari√ß√£o p√∫blica do reitor"
‚Üí Usa: semantic_search(profile="reitor") + get_recent_posts
‚Üí Retorna: Post mais recente relevante
```

#### üé≠ An√°lise de Sentimento

```
‚úÖ "Como o reitor √© visto pelos estudantes?"
‚Üí Usa: analyze_sentiment(topic="reitor", profile="dceuff")
‚Üí Retorna:
  ‚Ä¢ 60% negativos, 25% positivos, 15% neutros
  ‚Ä¢ Aspectos positivos: transpar√™ncia, di√°logo
  ‚Ä¢ Cr√≠ticas: demora, falta de comunica√ß√£o
  ‚Ä¢ Resumo narrativo + exemplos

‚úÖ "Qual a percep√ß√£o sobre o HUAP?"
‚Üí Usa: analyze_sentiment(topic="HUAP")
‚Üí Retorna: An√°lise completa de sentimento

‚úÖ "O que pensam sobre a gest√£o?"
‚Üí Usa: analyze_sentiment(topic="gest√£o")
‚Üí Retorna: Opini√µes e tend√™ncias identificadas
```

#### üìà Estat√≠sticas

```
‚úÖ "Estat√≠sticas do DCE"
‚Üí Usa: get_profile_statistics(profile="dceuff")
‚Üí Retorna:
  ‚Ä¢ 1.503 posts
  ‚Ä¢ 45.678 curtidas totais
  ‚Ä¢ M√©dia: 30.4 curtidas/post
  ‚Ä¢ Post mais engajado

‚úÖ "Posts da √∫ltima semana"
‚Üí Usa: get_recent_posts(days=7)
‚Üí Retorna: Todos os posts recentes

‚úÖ "Top 5 posts com mais coment√°rios"
‚Üí Usa: get_top_posts_by_comments(limit=5)
‚Üí Retorna: Ranking com links
```

---

## üõ†Ô∏è Ferramentas Dispon√≠veis

### 1. get_top_posts_by_likes
**Uso:** Encontrar posts mais curtidos  
**Par√¢metros:**
- `limit` (int): Quantidade de posts
- `profile` (str, opcional): Filtrar por perfil

**Exemplo:**
```python
tools.get_top_posts_by_likes(limit=10, profile="reitor")
```

### 2. get_top_posts_by_comments
**Uso:** Posts com mais coment√°rios  
**Par√¢metros:**
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 3. get_posts_by_engagement
**Uso:** Maior engajamento (curtidas + coment√°rios)  
**Par√¢metros:**
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 4. get_recent_posts
**Uso:** Publica√ß√µes recentes  
**Par√¢metros:**
- `days` (int): √öltimos N dias
- `limit` (int): Quantidade
- `profile` (str, opcional): Perfil

### 5. get_profile_statistics
**Uso:** Estat√≠sticas agregadas de um perfil  
**Par√¢metros:**
- `profile` (str, opcional): Se vazio, retorna todos

**Retorna:**
```json
{
  "total_posts": 1503,
  "total_likes": 45678,
  "total_comments": 2341,
  "avg_likes_per_post": 30.4,
  "avg_comments_per_post": 1.6,
  "total_engagement": 48019,
  "top_post": {...}
}
```

### 6. compare_profiles
**Uso:** Comparar todos os perfis  
**Sem par√¢metros**

**Retorna:**
```json
{
  "dceuff": {
    "total_posts": 1503,
    "total_likes": 45678,
    "avg_likes": 30.4,
    ...
  },
  "reitor": {...},
  "vicereitor": {...}
}
```

### 7. count_term_occurrences ‚≠ê NOVO
**Uso:** Quantificar men√ß√µes de um termo  
**Par√¢metros:**
- `term` (str): Termo a buscar
- `profile` (str, opcional): Perfil
- `case_sensitive` (bool): Mai√∫sculas/min√∫sculas

**Retorna:**
```json
{
  "count": 42,
  "percentage": 1.74,
  "total_posts": 2413,
  "term": "greve",
  "matching_posts": [...]
}
```

**Diferen√ßa de semantic_search:**
- `count_term_occurrences`: **QUANTIFICA** (todos os posts)
- `semantic_search`: **QUALIFICA** (posts mais relevantes)

### 8. analyze_sentiment ‚≠ê NOVO - IA
**Uso:** An√°lise de sentimento com LLM  
**Par√¢metros:**
- `topic` (str): T√≥pico/entidade
- `profile` (str, opcional): Perfil
- `n_posts` (int): Posts a analisar (padr√£o: 20)

**Retorna:**
```json
{
  "topic": "reitor",
  "sentiment_summary": "An√°lise narrativa...",
  "positive_count": 5,
  "negative_count": 12,
  "neutral_count": 3,
  "positive_aspects": ["transpar√™ncia", "di√°logo"],
  "negative_aspects": ["demora", "comunica√ß√£o"],
  "key_points": [...],
  "examples": {
    "positive": [...],
    "negative": [...],
    "neutral": [...]
  }
}
```

**Como funciona:**
1. Busca posts que mencionam o t√≥pico
2. Seleciona at√© N posts para an√°lise
3. LLM analisa e classifica cada post
4. Extrai aspectos positivos e negativos
5. Gera resumo qualitativo
6. Retorna estat√≠sticas + exemplos

### 9. semantic_search
**Uso:** Busca vetorial por conte√∫do  
**Par√¢metros:**
- `query` (str): Consulta sem√¢ntica
- `n_results` (int): Quantidade
- `profile` (str, opcional): Perfil

**Como funciona:**
- Converte query em embedding
- Busca posts similares no espa√ßo vetorial
- Retorna os N mais relevantes

---

## üåê API REST

A aplica√ß√£o Gradio exp√µe uma API REST autom√°tica.

### Endpoint Principal

```
POST http://localhost:7860/api/predict
```

### Fazer uma Pergunta

```bash
curl -X POST http://localhost:7860/api/predict \
  -H "Content-Type: application/json" \
  -d '{
    "data": [
      "Quantos posts falam de greve?",
      [],
      5,
      "üåê Todos os Perfis"
    ]
  }'
```

**Par√¢metros (array `data`):**
1. Pergunta (string)
2. Hist√≥rico do chat (array, pode ser `[]`)
3. N√∫mero de resultados (int, ignorado no modo agente)
4. Filtro de perfil (string: "üåê Todos os Perfis", "@dceuff", "@reitor", "@vicereitor")

### Resposta

```json
{
  "data": [
    "",  // Input vazio (limpo ap√≥s envio)
    [    // Hist√≥rico atualizado
      [
        "Quantos posts falam de greve?",
        "Encontrados 42 posts (1.74%) que mencionam 'greve'..."
      ]
    ],
    "<div>...</div>"  // HTML dos posts recuperados
  ],
  "duration": 2.34
}
```

### Exemplo com Python

```python
import requests

response = requests.post(
    "http://localhost:7860/api/predict",
    json={
        "data": [
            "Como o reitor √© visto?",
            [],
            5,
            "@dceuff"
        ]
    }
)

result = response.json()
answer = result['data'][1][0][1]  # Resposta do bot
print(answer)
```

---

## üìö Documenta√ß√£o Completa

### Arquivos de Documenta√ß√£o

Toda a documenta√ß√£o est√° consolidada aqui, mas arquivos individuais ainda existem:

| Arquivo | Conte√∫do |
|---------|----------|
| `README.md` | **Este arquivo** - Documenta√ß√£o completa |
| `QUICKSTART.md` | Guia r√°pido de in√≠cio |
| `API_QUICKSTART.md` | Exemplos de uso da API |
| `TOOLS.md` | Detalhes de todas as ferramentas |
| `SENTIMENT_ANALYSIS_TOOL.md` | An√°lise de sentimento (ferramenta #8) |
| `TERM_COUNT_TOOL.md` | Contagem de termos (ferramenta #7) |
| `ARCHITECTURE.md` | Arquitetura detalhada |
| `AGENT_VS_CLASSIC.md` | Compara√ß√£o agente vs sistema cl√°ssico |
| `BALANCED_AGENT.md` | Como o agente equilibra ferramentas |

### Estrutura de Arquivos

```
ping/
‚îú‚îÄ‚îÄ üìÅ data/                      # Dados dos posts (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ dceuff.json              # 1.503 posts
‚îÇ   ‚îú‚îÄ‚îÄ reitor.json              # 575 posts
‚îÇ   ‚îî‚îÄ‚îÄ vicereitor.json          # 335 posts
‚îÇ
‚îú‚îÄ‚îÄ üìÅ chroma_db/                # Banco vetorial (auto-gerado)
‚îÇ   ‚îî‚îÄ‚îÄ ...                      # Embeddings persistidos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ assets/                   # Assets da interface
‚îÇ   ‚îî‚îÄ‚îÄ agent_avatar.png         # Avatar do agente
‚îÇ
‚îú‚îÄ‚îÄ üêç C√ìDIGO PRINCIPAL
‚îÇ   ‚îú‚îÄ‚îÄ app.py                   # Interface Gradio
‚îÇ   ‚îú‚îÄ‚îÄ agent_system.py          # Sistema de agente RAG
‚îÇ   ‚îú‚îÄ‚îÄ query_tools.py           # 9 ferramentas especializadas
‚îÇ   ‚îú‚îÄ‚îÄ embedding_manager.py     # Gerenciador ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py           # Carregador de dados
‚îÇ   ‚îî‚îÄ‚îÄ rag_system.py            # Sistema RAG cl√°ssico (legado)
‚îÇ
‚îú‚îÄ‚îÄ üìÑ CONFIGURA√á√ÉO
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml           # Depend√™ncias (uv)
‚îÇ   ‚îî‚îÄ‚îÄ .python-version          # Python 3.12+
‚îÇ
‚îú‚îÄ‚îÄ üß™ TESTES
‚îÇ   ‚îú‚îÄ‚îÄ test_term_count.py       # Teste de contagem
‚îÇ   ‚îî‚îÄ‚îÄ check_profiles.py        # Debug de perfis
‚îÇ
‚îî‚îÄ‚îÄ üìö DOCUMENTA√á√ÉO
    ‚îú‚îÄ‚îÄ README.md                # ‚Üê VOC√ä EST√Å AQUI
    ‚îú‚îÄ‚îÄ QUICKSTART.md
    ‚îú‚îÄ‚îÄ API_QUICKSTART.md
    ‚îú‚îÄ‚îÄ TOOLS.md
    ‚îú‚îÄ‚îÄ SENTIMENT_ANALYSIS_TOOL.md
    ‚îú‚îÄ‚îÄ TERM_COUNT_TOOL.md
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md
    ‚îú‚îÄ‚îÄ AGENT_VS_CLASSIC.md
    ‚îî‚îÄ‚îÄ BALANCED_AGENT.md
```

### Tecnologias Utilizadas

| Tecnologia | Vers√£o | Uso |
|-----------|--------|-----|
| **Python** | 3.12+ | Linguagem principal |
| **uv** | Latest | Gerenciador de pacotes |
| **Ollama** | Latest | Runtime para LLMs locais |
| **ChromaDB** | Latest | Banco de dados vetorial |
| **Gradio** | 4.0+ | Interface web |
| **mxbai-embed-large** | 669MB | Modelo de embeddings |
| **qwen3:30b** | 18GB | Modelo de gera√ß√£o (padr√£o) |

---

## üêõ Solu√ß√£o de Problemas

### Erro: "Model not found"

```bash
# Verifique modelos instalados
ollama list

# Instale o modelo necess√°rio
ollama pull mxbai-embed-large
ollama pull qwen3:30b
```

### Erro: "Connection refused" (Ollama)

```bash
# Verifique se Ollama est√° rodando
ollama list

# Se n√£o estiver, inicie:
ollama serve

# Ou no sistema
ps aux | grep ollama
```

### ChromaDB n√£o persiste dados

```bash
# Verifique permiss√µes
chmod -R 755 chroma_db/

# Ou recrie do zero
rm -rf chroma_db/
uv run python app.py  # Reindexar√° automaticamente
```

### Interface n√£o mostra todos os perfis

```bash
# Verifique se os 3 perfis est√£o carregados
uv run python check_profiles.py

# Deve mostrar:
# Perfis encontrados: ['dceuff', 'reitor', 'vicereitor']

# Limpe cache do navegador (Ctrl+Shift+R)
```

### Mem√≥ria insuficiente

**Op√ß√£o 1: Use modelo menor**
```bash
uv run python app.py --generation-model qwen2.5:3b
```

**Op√ß√£o 2: Libere mem√≥ria**
- Feche outros aplicativos
- Reinicie Ollama: `killall ollama && ollama serve`

**Op√ß√£o 3: Configure swap (Linux)**
```bash
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### Respostas lentas

1. **Use modelo mais leve:**
   ```bash
   uv run python app.py --generation-model qwen2.5:7b
   ```

2. **Reduza posts analisados:**
   - An√°lise de sentimento: max 20 posts
   - Busca sem√¢ntica: max 10 posts

3. **Verifique GPU:**
   ```bash
   # Se tiver NVIDIA GPU
   nvidia-smi
   # Ollama usa GPU automaticamente
   ```

### Erro de parsing JSON (an√°lise de sentimento)

√Äs vezes o LLM retorna JSON malformado. O sistema tem fallback autom√°tico, mas voc√™ pode:

1. Usar modelo maior (qwen3:30b mais confi√°vel que qwen2.5:3b)
2. Reduzir n√∫mero de posts analisados
3. Tentar novamente (LLMs podem variar)

---

## üìä Performance e Benchmarks

### Tempos M√©dios (Hardware: 16GB RAM, qwen3:30b)

| Opera√ß√£o | Quantidade | Tempo | Cache |
|----------|-----------|-------|-------|
| Indexa√ß√£o inicial | 2.413 posts | ~8 min | N/A |
| Busca sem√¢ntica | 10 resultados | ~1-2s | Warm |
| Contagem de termo | Toda base | ~1-2s | N/A |
| An√°lise de sentimento | 20 posts | ~8-15s | N/A |
| Estat√≠sticas | 1 perfil | ~0.5s | N/A |
| S√≠ntese LLM | 1 resposta | ~3-8s | Warm |

### Uso de Recursos

| Componente | RAM | Disco | GPU |
|-----------|-----|-------|-----|
| ChromaDB | ~200MB | ~50MB | N√£o |
| mxbai-embed-large | ~700MB | 669MB | Opcional |
| qwen3:30b | ~18GB | 18GB | Sim* |
| Gradio | ~100MB | - | N√£o |
| **Total** | **~19GB** | **~19GB** | **Opcional** |

*GPU acelera significativamente (3-5x mais r√°pido)

---

## ü§ù Contribuindo

Melhorias s√£o bem-vindas! √Åreas de interesse:

### Funcionalidades
- [ ] An√°lise temporal (trends ao longo do tempo)
- [ ] Exporta√ß√£o de relat√≥rios (PDF, CSV)
- [ ] Visualiza√ß√µes (gr√°ficos, word clouds)
- [ ] Suporte a mais redes sociais
- [ ] Sistema de cache inteligente
- [ ] An√°lise de imagens (multimodal)

### Melhorias T√©cnicas
- [ ] Testes automatizados
- [ ] CI/CD pipeline
- [ ] Docker container
- [ ] Documenta√ß√£o em ingl√™s
- [ ] Logs estruturados
- [ ] Monitoring e m√©tricas

### Como Contribuir

1. Fork o reposit√≥rio
2. Crie uma branch (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add: MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

---

## üìÑ Licen√ßa

Este projeto √© de c√≥digo aberto para fins educacionais e institucionais.

---

## üôè Cr√©ditos

### Tecnologias

- **[Ollama](https://ollama.com/)** - Runtime para LLMs locais
- **[ChromaDB](https://www.trychroma.com/)** - Banco de dados vetorial
- **[Gradio](https://gradio.app/)** - Framework de interface web
- **[uv](https://docs.astral.sh/uv/)** - Gerenciador de pacotes Python moderno

### Modelos de IA

- **mxbai-embed-large** - Embeddings (mixedbread.ai)
- **qwen3:30b / qwen2.5** - Modelos de linguagem (Alibaba Cloud)

### Desenvolvimento

Desenvolvido para an√°lise de comunica√ß√£o institucional da **Universidade Federal Fluminense (UFF)**.

---

## üìû Suporte

- **Issues:** Abra uma issue no reposit√≥rio
- **Documenta√ß√£o:** Consulte os arquivos `.md` na raiz
- **Comunidade:** Compartilhe sua experi√™ncia e melhorias

---

<div align="center">

**[‚¨Ü Voltar ao topo](#-uff-instagram-analytics---sistema-rag-inteligente)**

---

Feito com ‚ù§Ô∏è para a comunidade UFF  
*Vers√£o 2.0 - Outubro 2025*

</div>
