"""
Sistema de Agente RAG com LLM que decide quais ferramentas usar.

O LLM recebe a pergunta do usuÃ¡rio e decide:
1. Se deve usar ferramentas estruturadas (query_tools)
2. Se deve usar busca semÃ¢ntica (RAG)
3. Quais parÃ¢metros usar
4. Como combinar mÃºltiplas ferramentas

Fluxo:
Usuario â†’ LLM Planejador â†’ Ferramentas â†’ LLM Sintetizador â†’ Resposta
"""

import json
from typing import Dict, Any, List, Tuple, Optional
import ollama

from query_tools import QueryTools
from embedding_manager import EmbeddingManager


class RAGAgent:
    """
    Agente inteligente que usa LLM para decidir quais ferramentas usar.
    """
    
    def __init__(
        self,
        embedding_model: str = "mxbai-embed-large",
        generation_model: str = "qwen3:30b",
        planning_model: str = "qwen3:30b"
    ):
        """
        Inicializa o agente RAG.
        
        Args:
            embedding_model: Modelo para embeddings
            generation_model: Modelo para gerar resposta final
            planning_model: Modelo para planejar aÃ§Ãµes (pode ser menor/mais rÃ¡pido)
        """
        self.embedding_manager = EmbeddingManager(embedding_model=embedding_model)
        self.query_tools = QueryTools(self.embedding_manager, llm_model=generation_model)
        self.generation_model = generation_model
        self.planning_model = planning_model
        
        print(f"âœ“ Agente RAG inicializado")
        print(f"  - Modelo de planejamento: {planning_model}")
        print(f"  - Modelo de geraÃ§Ã£o: {generation_model}")
    
    def _create_tools_description(self) -> str:
        """
        Cria descriÃ§Ã£o das ferramentas disponÃ­veis para o LLM.
        
        Returns:
            String com descriÃ§Ã£o de todas as ferramentas
        """
        return """
## FERRAMENTAS DISPONÃVEIS:

1. **get_top_posts_by_likes**
   - Uso: Encontrar posts com mais curtidas
   - ParÃ¢metros: limit (int), profile (str, opcional)
   - Exemplo: {"tool": "get_top_posts_by_likes", "limit": 10, "profile": "reitor"}

2. **get_top_posts_by_comments**
   - Uso: Encontrar posts com mais comentÃ¡rios
   - ParÃ¢metros: limit (int), profile (str, opcional)
   - Exemplo: {"tool": "get_top_posts_by_comments", "limit": 5}

3. **get_posts_by_engagement**
   - Uso: Encontrar posts com maior engajamento total (curtidas + comentÃ¡rios)
   - ParÃ¢metros: limit (int), profile (str, opcional)
   - Exemplo: {"tool": "get_posts_by_engagement", "limit": 10, "profile": "dceuff"}

4. **get_recent_posts**
   - Uso: Encontrar posts publicados recentemente
   - ParÃ¢metros: days (int), limit (int), profile (str, opcional)
   - Exemplo: {"tool": "get_recent_posts", "days": 7, "limit": 5}

5. **get_profile_statistics**
   - Uso: Obter estatÃ­sticas agregadas de um perfil
   - ParÃ¢metros: profile (str, opcional - se omitido, retorna todos)
   - Exemplo: {"tool": "get_profile_statistics", "profile": "reitor"}

6. **compare_profiles**
   - Uso: Comparar estatÃ­sticas entre todos os perfis
   - ParÃ¢metros: nenhum
   - Exemplo: {"tool": "compare_profiles"}

7. **count_term_occurrences**
   - Uso: QUANTIFICAR quantos posts mencionam um termo especÃ­fico (consulta TODA a base)
   - Quando usar: "quantos posts falam sobre X", "quantas vezes mencionaram Y", "frequÃªncia de Z"
   - ParÃ¢metros: term (str - termo a buscar), profile (str, opcional), case_sensitive (bool, default=False)
   - Exemplo: {"tool": "count_term_occurrences", "term": "greve", "profile": "dceuff"}
   - IMPORTANTE: Esta ferramenta CONTA ocorrÃªncias, nÃ£o retorna os posts mais relevantes

8. **analyze_sentiment**
   - Uso: ANALISAR SENTIMENTO e percepÃ§Ã£o sobre um tÃ³pico/entidade usando LLM
   - Quando usar: "como Ã© visto X?", "percepÃ§Ã£o sobre Y", "o que pensam sobre Z", "anÃ¡lise de sentimento", "avaliaÃ§Ã£o de X"
   - ParÃ¢metros: topic (str - tÃ³pico/entidade), profile (str, opcional), n_posts (int, default=20)
   - Exemplo: {"tool": "analyze_sentiment", "topic": "reitor", "profile": "dceuff", "n_posts": 20}
   - Retorna: Contagem positivo/negativo/neutro, aspectos positivos/negativos, resumo qualitativo

9. **semantic_search**
   - Uso: Buscar posts por CONTEÃšDO/TEMA usando busca semÃ¢ntica vetorial
   - Quando usar: Perguntas sobre "o que foi dito", "posts sobre X", "apariÃ§Ãµes", "mencionou", etc.
   - ParÃ¢metros: query (str - reformule para otimizar busca), n_results (int), profile (str, opcional)
   - Exemplo: {"tool": "semantic_search", "query": "HUAP hospital atendimento saÃºde", "n_results": 8}
   - IMPORTANTE: Reformule a query do usuÃ¡rio para termos mais especÃ­ficos e relevantes

## PERFIS DISPONÃVEIS:
- dceuff (DiretÃ³rio Central dos Estudantes)
- reitor (Reitor da UFF)
- vicereitor (Vice-Reitor da UFF)

## DIRETRIZES CRÃTICAS:

### Use SEMANTIC_SEARCH quando:
âœ… Pergunta sobre CONTEÃšDO: "o que foi dito sobre X", "posts que mencionam Y", "falar sobre Z"
âœ… Busca por TEMA: "apariÃ§Ãµes pÃºblicas", "eventos", "anÃºncios", "opiniÃµes sobre"
âœ… Perguntas ABERTAS: "como X tratou Y", "qual posicionamento sobre Z"
âœ… TEMPORAL + CONTEÃšDO: "o que foi dito em 2024 sobre X"
âœ… Contexto especÃ­fico: "Ãºltima apariÃ§Ã£o pÃºblica", "pronunciamento sobre"

### Use FERRAMENTAS ESTRUTURADAS quando:
âœ… RANKING: "mais curtidos", "top 10", "maior engajamento"
âœ… MÃ‰TRICAS: "quantos posts", "mÃ©dia de curtidas", "estatÃ­sticas"
âœ… COMPARAÃ‡Ã•ES NUMÃ‰RICAS: "qual perfil tem mais X"
âœ… FILTROS TEMPORAIS PUROS: "posts da Ãºltima semana" (sem contexto de conteÃºdo)
âœ… QUANTIFICAÃ‡ÃƒO DE TERMOS: "quantos posts falam sobre X", "frequÃªncia de Y" (use count_term_occurrences)
âœ… ANÃLISE DE SENTIMENTO: "como Ã© visto X?", "percepÃ§Ã£o sobre Y", "opiniÃ£o sobre Z" (use analyze_sentiment)

### COMBINE FERRAMENTAS quando:
âœ… Pergunta tem MÃ‰TRICA + CONTEÃšDO: use semantic_search primeiro, depois filtre por mÃ©trica
âœ… Precisa de CONTEXTO + RANKING: busca semÃ¢ntica + ordenaÃ§Ã£o

## INSTRUÃ‡Ã•ES:
- Analise se a pergunta Ã© sobre CONTEÃšDO (use semantic_search) ou MÃ‰TRICAS (use tools estruturadas)
- VocÃª pode usar MÃšLTIPLAS ferramentas em sequÃªncia
- Ao usar semantic_search, REFORMULE a query para termos mais especÃ­ficos
- Retorne APENAS um JSON vÃ¡lido, sem texto adicional
"""

    def _plan_action(
        self,
        user_question: str,
        profile_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        LLM decide quais ferramentas usar e com quais parÃ¢metros.
        
        Args:
            user_question: Pergunta do usuÃ¡rio
            profile_filter: Filtro de perfil (opcional)
            
        Returns:
            Lista de aÃ§Ãµes (ferramentas) a executar
        """
        tools_desc = self._create_tools_description()
        
        planning_prompt = f"""{tools_desc}

## CONTEXTO:
Pergunta do usuÃ¡rio: "{user_question}"
{f'Perfil filtrado: {profile_filter}' if profile_filter else 'Sem filtro de perfil'}

## SUA TAREFA:
Analise a pergunta e decida qual(is) ferramenta(s) usar para respondÃª-la da melhor forma.

RETORNE APENAS UM JSON no seguinte formato:
{{
    "reasoning": "Breve explicaÃ§Ã£o do seu raciocÃ­nio",
    "actions": [
        {{"tool": "nome_da_ferramenta", "params": {{"param1": "valor1"}}}}
    ]
}}

EXEMPLOS:

Pergunta: "Quais foram os 5 posts mais curtidos?"
{{
    "reasoning": "Ranking numÃ©rico por curtidas - mÃ©trica pura",
    "actions": [
        {{"tool": "get_top_posts_by_likes", "params": {{"limit": 5}}}}
    ]
}}

Pergunta: "Me fale sobre posts do HUAP"
{{
    "reasoning": "Pergunta sobre CONTEÃšDO especÃ­fico (HUAP), precisa busca semÃ¢ntica",
    "actions": [
        {{"tool": "semantic_search", "params": {{"query": "HUAP hospital universitÃ¡rio atendimento saÃºde", "n_results": 8}}}}
    ]
}}

Pergunta: "Qual foi a Ãºltima apariÃ§Ã£o pÃºblica do reitor?"
{{
    "reasoning": "Pergunta sobre CONTEÃšDO (apariÃ§Ã£o pÃºblica) com filtro temporal e de perfil. Usar busca semÃ¢ntica com query otimizada",
    "actions": [
        {{"tool": "semantic_search", "params": {{"query": "reitor apariÃ§Ã£o pÃºblica evento pronunciamento presenÃ§a cerimÃ´nia", "n_results": 10, "profile": "reitor"}}}}
    ]
}}

Pergunta: "O que estÃ£o falando do reitor em 2024?"
{{
    "reasoning": "Pergunta sobre CONTEÃšDO relacionado ao reitor. Busca semÃ¢ntica com filtro de perfil",
    "actions": [
        {{"tool": "semantic_search", "params": {{"query": "reitor UFF aÃ§Ãµes gestÃ£o decisÃµes anÃºncios", "n_results": 10}}}}
    ]
}}

Pergunta: "Compare o engajamento entre reitor e DCE"
{{
    "reasoning": "ComparaÃ§Ã£o de MÃ‰TRICAS estatÃ­sticas entre perfis",
    "actions": [
        {{"tool": "compare_profiles", "params": {{}}}}
    ]
}}

Pergunta: "Qual foi o post mais curtido recente do reitor?"
{{
    "reasoning": "Combina MÃ‰TRICA (curtidas) + temporal (recente) + perfil. Ferramenta estruturada resolve",
    "actions": [
        {{"tool": "get_top_posts_by_likes", "params": {{"limit": 1, "profile": "reitor"}}}}
    ]
}}

Pergunta: "Posts sobre pesquisa cientÃ­fica que tiveram mais engajamento"
{{
    "reasoning": "Combina CONTEÃšDO (pesquisa) + MÃ‰TRICA (engajamento). Usar busca semÃ¢ntica primeiro",
    "actions": [
        {{"tool": "semantic_search", "params": {{"query": "pesquisa cientÃ­fica ciÃªncia laboratÃ³rio estudo investigaÃ§Ã£o", "n_results": 20}}}},
        {{"tool": "get_posts_by_engagement", "params": {{"limit": 5}}}}
    ]
}}

Pergunta: "O que o DCE publicou sobre greve na Ãºltima semana?"
{{
    "reasoning": "CONTEÃšDO (greve) + temporal (Ãºltima semana) + perfil. Busca semÃ¢ntica com filtro temporal",
    "actions": [
        {{"tool": "get_recent_posts", "params": {{"days": 7, "limit": 30, "profile": "dceuff"}}}},
        {{"tool": "semantic_search", "params": {{"query": "greve paralisaÃ§Ã£o mobilizaÃ§Ã£o protesto reivindicaÃ§Ã£o", "n_results": 10, "profile": "dceuff"}}}}
    ]
}}

IMPORTANTE: 
- Para CONTEÃšDO/TEMA â†’ semantic_search (sempre reformule query)
- Para MÃ‰TRICAS/RANKING â†’ ferramentas estruturadas
- Retorne APENAS o JSON, nada mais!
"""

        try:
            # Chama o LLM para planejar
            response = ollama.chat(
                model=self.planning_model,
                messages=[
                    {
                        'role': 'system',
                        'content': 'VocÃª Ã© um planejador especializado em anÃ¡lise de dados do Instagram. Retorne APENAS JSON vÃ¡lido, sem markdown ou texto adicional.'
                    },
                    {
                        'role': 'user',
                        'content': planning_prompt
                    }
                ],
                format='json'  # ForÃ§a saÃ­da em JSON
            )
            
            # Parse da resposta
            plan_text = response['message']['content']
            
            # Tenta parsear o JSON
            try:
                plan = json.loads(plan_text)
            except json.JSONDecodeError:
                # Se falhar, tenta extrair JSON do texto
                import re
                json_match = re.search(r'\{.*\}', plan_text, re.DOTALL)
                if json_match:
                    plan = json.loads(json_match.group())
                else:
                    raise ValueError("LLM nÃ£o retornou JSON vÃ¡lido")
            
            print(f"\nğŸ¤” RaciocÃ­nio do agente: {plan.get('reasoning', 'N/A')}")
            print(f"ğŸ”§ AÃ§Ãµes planejadas: {len(plan.get('actions', []))} ferramenta(s)")
            
            return plan.get('actions', [])
            
        except Exception as e:
            print(f"âš ï¸ Erro no planejamento: {e}")
            # Fallback: busca semÃ¢ntica simples
            return [{
                'tool': 'semantic_search',
                'params': {
                    'query': user_question,
                    'n_results': 5,
                    'profile': profile_filter
                }
            }]
    
    def _execute_action(
        self,
        action: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Executa uma aÃ§Ã£o (ferramenta).
        
        Args:
            action: DicionÃ¡rio com tool e params
            
        Returns:
            Resultados da ferramenta
        """
        tool = action.get('tool')
        params = action.get('params', {})
        
        print(f"  âš™ï¸ Executando: {tool} com params {params}")
        
        try:
            if tool == 'get_top_posts_by_likes':
                return self.query_tools.get_top_posts_by_likes(
                    limit=params.get('limit', 10),
                    profile=params.get('profile')
                )
            
            elif tool == 'get_top_posts_by_comments':
                return self.query_tools.get_top_posts_by_comments(
                    limit=params.get('limit', 10),
                    profile=params.get('profile')
                )
            
            elif tool == 'get_posts_by_engagement':
                return self.query_tools.get_posts_by_engagement(
                    limit=params.get('limit', 10),
                    profile=params.get('profile')
                )
            
            elif tool == 'get_recent_posts':
                return self.query_tools.get_recent_posts(
                    days=params.get('days', 30),
                    limit=params.get('limit', 10),
                    profile=params.get('profile')
                )
            
            elif tool == 'get_profile_statistics':
                stats = self.query_tools.get_profile_statistics(
                    profile=params.get('profile')
                )
                return [{'metadata': stats, 'is_stats': True}]
            
            elif tool == 'compare_profiles':
                comparison = self.query_tools.compare_profiles()
                return [{'metadata': comparison, 'is_comparison': True}]
            
            elif tool == 'count_term_occurrences':
                result = self.query_tools.count_term_occurrences(
                    term=params.get('term', ''),
                    profile=params.get('profile'),
                    case_sensitive=params.get('case_sensitive', False)
                )
                return [{'metadata': result, 'is_term_count': True}]
            
            elif tool == 'analyze_sentiment':
                result = self.query_tools.analyze_sentiment(
                    topic=params.get('topic', ''),
                    profile=params.get('profile'),
                    n_posts=params.get('n_posts', 20)
                )
                return [{'metadata': result, 'is_sentiment': True}]
            
            elif tool == 'semantic_search':
                query = params.get('query', '')
                n_results = params.get('n_results', 5)
                profile = params.get('profile')
                
                # Busca no embedding manager
                raw_results = self.embedding_manager.search(
                    query=query,
                    n_results=n_results,
                    profile_filter=profile
                )
                
                # Converte para formato padrÃ£o (lista de dicts)
                if not raw_results or 'ids' not in raw_results or not raw_results['ids']:
                    return []
                
                formatted_results = []
                for i in range(len(raw_results['ids'][0])):
                    formatted_results.append({
                        'id': raw_results['ids'][0][i],
                        'metadata': raw_results['metadatas'][0][i],
                        'document': raw_results['documents'][0][i],
                        'distance': raw_results['distances'][0][i] if 'distances' in raw_results else None
                    })
                
                return formatted_results
            
            else:
                print(f"âš ï¸ Ferramenta desconhecida: {tool}")
                return []
        
        except Exception as e:
            print(f"âŒ Erro ao executar {tool}: {e}")
            return []
    
    def _format_results_for_llm(
        self,
        results: List[Dict[str, Any]],
        tool_name: str
    ) -> str:
        """
        Formata resultados das ferramentas para o LLM sintetizar.
        
        Args:
            results: Resultados das ferramentas
            tool_name: Nome da ferramenta usada
            
        Returns:
            String formatada com os resultados
        """
        if not results:
            return f"## Resultado de {tool_name}:\nNenhum resultado encontrado."
        
        # Verifica se Ã© estatÃ­sticas
        if results[0].get('is_stats'):
            stats = results[0]['metadata']
            return f"""## EstatÃ­sticas:
- Total de posts: {stats.get('total_posts', 0)}
- Total de curtidas: {stats.get('total_likes', 0)}
- Total de comentÃ¡rios: {stats.get('total_comments', 0)}
- MÃ©dia de curtidas por post: {stats.get('avg_likes_per_post', 0):.2f}
- MÃ©dia de comentÃ¡rios por post: {stats.get('avg_comments_per_post', 0):.2f}
- Engajamento total: {stats.get('total_engagement', 0)}
{f"- Post mais engajado: {stats['top_post']['url']} ({stats['top_post']['engagement']} engajamentos)" if stats.get('top_post') else ''}
"""
        
        # Verifica se Ã© comparaÃ§Ã£o
        if results[0].get('is_comparison'):
            comparison = results[0]['metadata']
            text = "## ComparaÃ§Ã£o Entre Perfis:\n\n"
            for profile, stats in comparison.items():
                text += f"### @{profile}\n"
                text += f"- Posts: {stats['total_posts']}\n"
                text += f"- Curtidas: {stats['total_likes']} (mÃ©dia: {stats['avg_likes']:.1f})\n"
                text += f"- ComentÃ¡rios: {stats['total_comments']} (mÃ©dia: {stats['avg_comments']:.1f})\n"
                text += f"- Engajamento total: {stats['total_engagement']}\n\n"
            return text
        
        # Verifica se Ã© contagem de termo
        if results[0].get('is_term_count'):
            data = results[0]['metadata']
            text = f"## Contagem de Termo: '{data['term']}'\n"
            text += f"- Perfil(s): {data['profile']}\n"
            text += f"- Posts encontrados: {data['count']} de {data['total_posts']} ({data['percentage']}%)\n\n"
            
            # Se houver erro
            if data.get('error'):
                text += f"âš ï¸ Erro: {data['error']}\n"
                return text
            
            # Lista alguns posts que contÃªm o termo
            if data['matching_posts']:
                text += "### Exemplos de posts que mencionam o termo:\n\n"
                for i, post in enumerate(data['matching_posts'][:5], 1):
                    meta = post.get('metadata', {})
                    doc = post.get('document', '')
                    text += f"**Post {i}** (@{meta.get('profile', 'unknown')})\n"
                    text += f"- Curtidas: {meta.get('likesCount', 0)}, ComentÃ¡rios: {meta.get('commentsCount', 0)}\n"
                    text += f"- Data: {meta.get('timestamp', 'N/A')[:10]}\n"
                    text += f"- Link: {meta.get('url', 'N/A')}\n"
                    if doc:
                        # Mostra trecho com o termo
                        text += f"- Trecho: {doc[:250]}...\n"
                    text += "\n"
            
            return text
        
        # Verifica se Ã© anÃ¡lise de sentimento
        if results[0].get('is_sentiment'):
            data = results[0]['metadata']
            text = f"## AnÃ¡lise de Sentimento: '{data['topic']}'\n"
            text += f"- Perfil(s): {data['profile']}\n"
            text += f"- Posts analisados: {data['total_posts']}\n\n"
            
            # Se houver erro
            if data.get('error'):
                text += f"âš ï¸ Erro: {data['error']}\n"
                return text
            
            # Resumo do sentimento
            text += f"### Resumo Geral:\n{data['sentiment_summary']}\n\n"
            
            # DistribuiÃ§Ã£o de sentimentos
            text += "### DistribuiÃ§Ã£o de Sentimentos:\n"
            text += f"- âœ… Positivos: {data['positive_count']} posts\n"
            text += f"- âŒ Negativos: {data['negative_count']} posts\n"
            text += f"- âšª Neutros: {data['neutral_count']} posts\n\n"
            
            # Aspectos positivos
            if data.get('positive_aspects'):
                text += "### Aspectos Positivos Identificados:\n"
                for aspect in data['positive_aspects']:
                    text += f"- {aspect}\n"
                text += "\n"
            
            # Aspectos negativos
            if data.get('negative_aspects'):
                text += "### Aspectos Negativos/CrÃ­ticas:\n"
                for aspect in data['negative_aspects']:
                    text += f"- {aspect}\n"
                text += "\n"
            
            # Pontos-chave
            if data.get('key_points'):
                text += "### Pontos-Chave:\n"
                for point in data['key_points']:
                    text += f"- {point}\n"
                text += "\n"
            
            # Exemplos de posts
            examples = data.get('examples', {})
            if examples.get('positive'):
                text += "### Exemplos de Posts Positivos:\n"
                for i, post in enumerate(examples['positive'][:2], 1):
                    meta = post.get('metadata', {})
                    doc = post.get('document', '')[:200]
                    text += f"{i}. @{meta.get('profile')}: {doc}... [{meta.get('url')}]\n"
                text += "\n"
            
            if examples.get('negative'):
                text += "### Exemplos de Posts Negativos:\n"
                for i, post in enumerate(examples['negative'][:2], 1):
                    meta = post.get('metadata', {})
                    doc = post.get('document', '')[:200]
                    text += f"{i}. @{meta.get('profile')}: {doc}... [{meta.get('url')}]\n"
                text += "\n"
            
            return text
        
        # Posts regulares
        text = f"## Resultados de {tool_name}:\n\n"
        for i, post in enumerate(results[:10], 1):
            meta = post.get('metadata', {})
            doc = post.get('document', '')
            
            text += f"**Post {i}** (@{meta.get('profile', 'unknown')})\n"
            text += f"- Curtidas: {meta.get('likesCount', 0)}\n"
            text += f"- ComentÃ¡rios: {meta.get('commentsCount', 0)}\n"
            text += f"- Data: {meta.get('timestamp', 'N/A')[:10]}\n"
            text += f"- Link: {meta.get('url', 'N/A')}\n"
            
            # Adiciona trecho do texto/legenda
            if doc:
                text += f"- ConteÃºdo: {doc[:200]}...\n"
            elif meta.get('caption'):
                text += f"- Legenda: {meta['caption'][:200]}...\n"
            
            text += "\n"
        
        return text
    
    def _synthesize_response(
        self,
        user_question: str,
        all_results: List[Tuple[str, List[Dict[str, Any]]]],
        stream: bool = False
    ) -> str:
        """
        LLM sintetiza todos os resultados em uma resposta coerente.
        
        Args:
            user_question: Pergunta original do usuÃ¡rio
            all_results: Lista de (nome_ferramenta, resultados)
            stream: Se True, retorna generator para streaming
            
        Returns:
            Resposta final sintetizada
        """
        # Monta contexto com todos os resultados
        context = ""
        for tool_name, results in all_results:
            context += self._format_results_for_llm(results, tool_name)
            context += "\n---\n\n"
        
        synthesis_prompt = f"""{context}

## Pergunta Original do UsuÃ¡rio:
{user_question}

## Sua Tarefa:
Sintetize os dados acima em uma resposta clara, completa e bem formatada para o usuÃ¡rio.

DIRETRIZES:
1. Use APENAS os dados fornecidos acima
2. Seja objetivo e direto
3. Cite nÃºmeros especÃ­ficos e links quando relevante
4. Use formataÃ§Ã£o markdown para melhor legibilidade
5. Organize a informaÃ§Ã£o de forma lÃ³gica
6. Se mÃºltiplas ferramentas foram usadas, integre os resultados de forma coerente
7. Sempre inclua links dos posts quando disponÃ­vel

NÃƒO invente informaÃ§Ãµes que nÃ£o estÃ£o no contexto!
"""

        try:
            response = ollama.chat(
                model=self.generation_model,
                messages=[
                    {
                        'role': 'system',
                        'content': 'VocÃª Ã© um assistente especializado em anÃ¡lise de posts do Instagram da UFF. Responda de forma clara, objetiva e bem formatada.'
                    },
                    {
                        'role': 'user',
                        'content': synthesis_prompt
                    }
                ],
                stream=stream
            )
            
            if stream:
                return response
            else:
                return response['message']['content']
        
        except Exception as e:
            return f"âŒ Erro ao gerar resposta: {e}"
    
    def query(
        self,
        question: str,
        profile_filter: Optional[str] = None,
        stream: bool = False
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """
        Processa query completa usando o agente inteligente.
        
        Fluxo:
        1. LLM planeja quais ferramentas usar
        2. Executa as ferramentas
        3. LLM sintetiza os resultados
        
        Args:
            question: Pergunta do usuÃ¡rio
            profile_filter: Filtro de perfil (opcional)
            stream: Streaming da resposta
            
        Returns:
            Tupla (resposta, todos_os_posts_recuperados)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Nova consulta: {question}")
        if profile_filter:
            print(f"ğŸ‘¤ Perfil: {profile_filter}")
        print(f"{'='*60}\n")
        
        # Fase 1: Planejamento
        print("ğŸ“‹ Fase 1: Planejamento de aÃ§Ãµes...")
        actions = self._plan_action(question, profile_filter)
        
        if not actions:
            return "NÃ£o consegui determinar como responder sua pergunta. Tente reformular.", []
        
        # Fase 2: ExecuÃ§Ã£o
        print(f"\nâš™ï¸ Fase 2: Executando {len(actions)} aÃ§Ã£o(Ãµes)...")
        all_results = []
        all_posts = []
        
        for i, action in enumerate(actions, 1):
            print(f"\n  AÃ§Ã£o {i}/{len(actions)}:")
            results = self._execute_action(action)
            
            if results:
                tool_name = action.get('tool', 'unknown')
                all_results.append((tool_name, results))
                all_posts.extend(results)
                print(f"  âœ“ {len(results)} resultado(s) obtido(s)")
            else:
                print(f"  âš ï¸ Nenhum resultado")
        
        if not all_results:
            return "NÃ£o encontrei informaÃ§Ãµes relevantes para sua pergunta.", []
        
        # Fase 3: SÃ­ntese
        print(f"\nğŸ¨ Fase 3: Sintetizando resposta final...")
        response = self._synthesize_response(
            user_question=question,
            all_results=all_results,
            stream=stream
        )
        
        print(f"\nâœ“ Resposta gerada!")
        print(f"{'='*60}\n")
        
        return response, all_posts


if __name__ == "__main__":
    """
    Teste do sistema de agente.
    """
    print("ğŸ§ª Testando Sistema de Agente RAG\n")
    
    # Inicializa agente
    agent = RAGAgent()
    
    # Testes diversos
    test_queries = [
        ("Qual foi o post mais curtido do reitor?", "reitor"),
        ("Me fale sobre posts do HUAP", None),
        ("Compare o engajamento entre os perfis", None),
        ("Quais foram os Ãºltimos 5 posts do DCE?", "dceuff"),
        ("EstatÃ­sticas do vice-reitor", "vicereitor"),
    ]
    
    for question, profile in test_queries:
        print(f"\n{'='*80}")
        response, posts = agent.query(question, profile_filter=profile)
        
        print(f"\nğŸ“„ RESPOSTA:")
        print(response)
        print(f"\nğŸ“Š Total de posts recuperados: {len(posts)}")
        
        input("\nâ–¶ï¸ Pressione ENTER para prÃ³ximo teste...")
