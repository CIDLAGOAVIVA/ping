# üöÄ Guia R√°pido de In√≠cio

## Instala√ß√£o em 3 Passos

### 1Ô∏è‚É£ Instalar Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh
```

### 2Ô∏è‚É£ Executar Setup

```bash
cd ping
./setup.sh
```

O script ir√°:
- ‚úÖ Verificar depend√™ncias
- ‚úÖ Instalar modelos necess√°rios
- ‚úÖ Sincronizar pacotes Python
- ‚úÖ Iniciar a aplica√ß√£o

### 3Ô∏è‚É£ Acessar Interface

Abra seu navegador em: **http://localhost:7860**

---

## Uso R√°pido

### Iniciar aplica√ß√£o (ap√≥s primeira configura√ß√£o)

```bash
./start.sh
```

### Iniciar com op√ß√µes

```bash
# Porta customizada
./start.sh --port 8080

# Criar link p√∫blico
./start.sh --share

# Modelo diferente
./start.sh --generation-model qwen2.5:7b
```

---

## Perguntas Exemplo

Digite no chat:

- "Quais posts do DCE UFF falam sobre eventos?"
- "Mostre os 5 posts mais curtidos"
- "O que o reitor publicou sobre o HUAP?"
- "Posts recentes com a palavra pesquisa"

---

## Solu√ß√£o R√°pida de Problemas

### Ollama n√£o est√° rodando
```bash
ollama serve
```

### Modelo n√£o encontrado
```bash
ollama pull mxbai-embed-large
ollama pull qwen2.5:3b
```

### Reindexar posts
```bash
rm -rf chroma_db/
./start.sh
```

---

## Arquitetura Simplificada

```
Pergunta ‚Üí Busca Vetorial ‚Üí Recupera Posts ‚Üí LLM gera Resposta
         (ChromaDB)         (Top 5)        (Ollama)
```

---

## Recursos do Sistema

| Componente | Tecnologia | Fun√ß√£o |
|------------|-----------|--------|
| Interface | Gradio | Chat web interativo |
| Embeddings | mxbai-embed-large | Vetoriza√ß√£o sem√¢ntica |
| Busca | ChromaDB | Banco de dados vetorial |
| Gera√ß√£o | Qwen/GPT-OSS | Resposta contextual |

---

## Pr√≥ximos Passos

1. ‚úÖ Testar perguntas b√°sicas
2. ‚úÖ Explorar filtros de perfil
3. ‚úÖ Ajustar n√∫mero de resultados
4. üîÑ Adicionar novos JSONs em `data/`
5. üîÑ Experimentar modelos maiores

---

Para mais detalhes, consulte o **[README.md](README.md)** completo.
